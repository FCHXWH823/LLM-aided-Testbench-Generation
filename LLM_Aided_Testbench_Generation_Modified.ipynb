{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1gL5kn5VUb2"
      },
      "source": [
        "# LLM-Aided Testbench Generation\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates a complete **LLM-aided testbench generation system** for Verilog hardware designs. The system automates the creation of comprehensive testbenches with golden reference outputs using a 5-step pipeline:\n",
        "\n",
        "1. **Step 1-2**: Accept natural language description and Verilog code\n",
        "2. **Step 3**: Generate testbench with comprehensive test patterns using LLM\n",
        "3. **Step 4**: Create Python golden model from description and compute expected outputs\n",
        "4. **Step 5**: Update testbench with verification logic and run simulation\n",
        "\n",
        "### Features\n",
        "\n",
        "- ü§ñ **LLM-Powered**: Uses GPT-4 to generate intelligent testbenches\n",
        "- üéØ **Comprehensive Testing**: Covers corner cases, boundary values, and random patterns\n",
        "- üîç **Automatic Verification**: Built-in pass/fail checking and test summaries\n",
        "- üìù **Self-Contained**: All code included in this notebook for easy execution\n",
        "- ‚ö° **Ready to Run**: Execute all cells to see the complete workflow\n",
        "\n",
        "This notebook is completely self-contained - you can run it directly without any external dependencies (except the `openai` library)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCkzAnZbVUb4"
      },
      "source": [
        "## Section 1: Installation and Setup\n",
        "\n",
        "First, we install the required dependencies and set up the environment. The only external dependency is the OpenAI API client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j67gMjpmVUb4",
        "outputId": "6b9c9a57-014d-43a8-c604-bd385b419e36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  gtkwave\n",
            "The following NEW packages will be installed:\n",
            "  iverilog\n",
            "0 upgraded, 1 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 2,130 kB of archives.\n",
            "After this operation, 6,749 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 iverilog amd64 11.0-1.1 [2,130 kB]\n",
            "Fetched 2,130 kB in 0s (16.9 MB/s)\n",
            "Selecting previously unselected package iverilog.\n",
            "(Reading database ... 126675 files and directories currently installed.)\n",
            "Preparing to unpack .../iverilog_11.0-1.1_amd64.deb ...\n",
            "Unpacking iverilog (11.0-1.1) ...\n",
            "Setting up iverilog (11.0-1.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "fatal: destination path 'LLM-aided-Testbench-Generation' already exists and is not an empty directory.\n",
            "‚úì Dependencies installed successfully\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install openai\n",
        "!apt-get install iverilog\n",
        "!git clone https://github.com/FCHXWH823/LLM-aided-Testbench-Generation.git\n",
        "\n",
        "# Import standard libraries\n",
        "import os\n",
        "import json\n",
        "import sys\n",
        "import subprocess\n",
        "from typing import Dict, Any, List, Optional\n",
        "import re\n",
        "\n",
        "print(\"‚úì Dependencies installed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz3JM_sxVUb5"
      },
      "source": [
        "## Section 2: API Key Configuration\n",
        "\n",
        "To use the full LLM-powered generation, you need to set your OpenAI API key. You can either:\n",
        "1. Set it as an environment variable: `export OPENAI_API_KEY='your-api-key'`\n",
        "2. Directly set it in the cell below\n",
        "\n",
        "**Note**: Without an API key, the system will run in mock/demo mode for demonstration purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxqlcj3oVUb5",
        "outputId": "f43a5dab-2fd3-4917-9505-26833b23e904"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì OpenAI API key is configured\n"
          ]
        }
      ],
      "source": [
        "# Set your OpenAI API key here (or use environment variable)\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "\n",
        "# Check if API key is set\n",
        "if os.environ['OPENAI_API_KEY']:\n",
        "    print(\"‚úì OpenAI API key is configured\")\n",
        "else:\n",
        "    print(\"‚ö† Warning: OpenAI API key not set. Running in demo mode.\")\n",
        "    print(\"  Set your API key with: os.environ['OPENAI_API_KEY'] = 'your-key'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc0LorUZVUb5"
      },
      "source": [
        "## Section 3: Create Output Directory\n",
        "\n",
        "We'll create a directory to store all generated files (testbenches, golden models, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04wFH0pTVUb5",
        "outputId": "725fd24d-8128-4120-9bae-b7b44531d549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Output directory created: LLM-aided-Testbench-Generation/examples/output\n"
          ]
        }
      ],
      "source": [
        "# Create output directory\n",
        "output_dir = \"LLM-aided-Testbench-Generation/examples/output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"‚úì Output directory created: {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--qCs7LJVUb5"
      },
      "source": [
        "## Section 4: LLM Client Implementation\n",
        "\n",
        "The `LLMClient` class handles all interactions with the OpenAI API. It provides a unified interface for generating text using GPT models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cpdFeyl6VUb5"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "LLM Client for interacting with language models.\n",
        "Supports multiple LLM providers (OpenAI, Anthropic, etc.)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "from typing import Optional, Dict, Any\n",
        "from xml.parsers.expat import model\n",
        "import openai\n",
        "\n",
        "\n",
        "class LLMClient:\n",
        "    \"\"\"Client for interacting with LLM APIs.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None, model: str = \"gpt-4\", provider: str = \"openai\"):\n",
        "        \"\"\"\n",
        "        Initialize LLM client.\n",
        "\n",
        "        Args:\n",
        "            api_key: API key for the LLM provider (if None, reads from environment)\n",
        "            model: Model name to use\n",
        "            provider: LLM provider ('openai', 'anthropic', etc.)\n",
        "        \"\"\"\n",
        "        self.provider = provider\n",
        "        self.model = model\n",
        "        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
        "        self.client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "    def generate(self, prompt: str, system_prompt: Optional[str] = None,\n",
        "                 temperature: float = 0.7, max_tokens: int = 4000) -> str:\n",
        "        \"\"\"\n",
        "        Generate text using the LLM.\n",
        "\n",
        "        Args:\n",
        "            prompt: User prompt\n",
        "            system_prompt: System prompt for the model\n",
        "            temperature: Sampling temperature (0-1)\n",
        "            max_tokens: Maximum tokens to generate\n",
        "\n",
        "        Returns:\n",
        "            Generated text response\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if self.provider == \"openai\":\n",
        "                messages = []\n",
        "                if system_prompt:\n",
        "                    messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "                messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=self.model,\n",
        "                    messages=messages,\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=max_tokens,\n",
        "                )\n",
        "                return response.choices[0].message.content\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported provider: {self.provider}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating response: {e}\")\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "    def is_available(self) -> bool:\n",
        "        \"\"\"Check if the LLM client is properly configured.\"\"\"\n",
        "        return self.api_key is not None and len(self.api_key) > 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfQNDg-OVUb6"
      },
      "source": [
        "## Section 5: Testbench Generator (Step 3)\n",
        "\n",
        "The `TestbenchGenerator` class generates Verilog testbenches with comprehensive test patterns. It:\n",
        "- Extracts module information (name, inputs, outputs) from Verilog code\n",
        "- Uses LLM to generate comprehensive test patterns covering corner cases, boundary values, and random values\n",
        "- Creates a testbench skeleton without expected outputs (those come in Step 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0rTlmXJmVUb6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Step 3: Generate testbench with test patterns (without golden outputs).\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "\n",
        "class TestbenchGenerator:\n",
        "    \"\"\"Generate Verilog testbench with test patterns using LLM.\"\"\"\n",
        "\n",
        "    def __init__(self, llm_client: LLMClient):\n",
        "        \"\"\"\n",
        "        Initialize testbench generator.\n",
        "\n",
        "        Args:\n",
        "            llm_client: LLM client instance\n",
        "        \"\"\"\n",
        "        self.llm_client = llm_client\n",
        "\n",
        "    def generate_testbench(self, description: str, verilog_code: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate testbench with comprehensive test patterns.\n",
        "\n",
        "        Args:\n",
        "            description: Natural language description of the Verilog module\n",
        "            verilog_code: Verilog code to be tested\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing:\n",
        "                - testbench_code: Verilog testbench code (without expected outputs)\n",
        "                - test_patterns: List of test input patterns\n",
        "                - module_info: Information about module (name, inputs, outputs)\n",
        "        \"\"\"\n",
        "        # First, extract module information\n",
        "        module_info = self._extract_module_info(verilog_code)\n",
        "\n",
        "        # Generate comprehensive test patterns\n",
        "        system_prompt = \"\"\"You are an expert in Verilog testbench generation.\n",
        "Your task is to generate comprehensive test patterns for a given Verilog module.\n",
        "Generate test patterns that cover:\n",
        "1. All corner cases\n",
        "2. Boundary values\n",
        "3. Typical use cases\n",
        "4. Edge cases\n",
        "5. Random values for thorough testing\"\"\"\n",
        "\n",
        "        user_prompt = f\"\"\"Given the following Verilog module and its natural language description,\n",
        "generate a comprehensive Verilog testbench that includes ALL possible test patterns.\n",
        "\n",
        "Natural Language Description:\n",
        "{description}\n",
        "\n",
        "Verilog Module Code:\n",
        "{verilog_code}\n",
        "\n",
        "Generate a Verilog testbench that:\n",
        "1. Declares all necessary signals\n",
        "2. Instantiates the module under test\n",
        "3. Includes a systematic set of test patterns covering all cases\n",
        "4. Uses $display to show inputs for each test (all $display statements are in the initial block and before $finish statement.)\n",
        "5. Does NOT include expected outputs or assertions yet (we will add those later)\n",
        "6. Numbers each test case\n",
        "\n",
        "Please provide:\n",
        "1. The complete testbench code\n",
        "2. A list of test patterns in JSON format with test number and input values\n",
        "\n",
        "Format your response as:\n",
        "TESTBENCH_CODE:\n",
        "```verilog\n",
        "[testbench code here]\n",
        "```\n",
        "\n",
        "TEST_PATTERNS (a list of dictionaries. Each dictionary contains only input signal names mapped to their values as plain binary strings (no prefixes like 0b, no spaces). Do not include any test_number field):\n",
        "```json\n",
        "[array of test patterns]\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "        response = self.llm_client.generate(user_prompt, system_prompt, max_tokens=4000)\n",
        "\n",
        "        # Parse the response\n",
        "        testbench_code = self._extract_section(response, \"TESTBENCH_CODE:\", \"```verilog\", \"```\")\n",
        "        test_patterns_json = self._extract_section(response, \"TEST_PATTERNS:\", \"```json\", \"```\")\n",
        "\n",
        "        try:\n",
        "            test_patterns = eval(test_patterns_json) if test_patterns_json else []\n",
        "        except:\n",
        "            test_patterns = []\n",
        "            print(\"Warning: Could not parse test patterns JSON\")\n",
        "\n",
        "        return {\n",
        "            \"testbench_code\": testbench_code,\n",
        "            \"test_patterns\": test_patterns,\n",
        "            \"module_info\": module_info,\n",
        "            \"raw_response\": response\n",
        "        }\n",
        "\n",
        "    def _extract_module_info(self, verilog_code: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Extract module information (name, inputs, outputs) from Verilog code.\n",
        "\n",
        "        Args:\n",
        "            verilog_code: Verilog module code\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with module information\n",
        "        \"\"\"\n",
        "        lines = verilog_code.strip().split('\\n')\n",
        "        module_name = \"\"\n",
        "        inputs = []\n",
        "        outputs = []\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line.startswith('module'):\n",
        "                # Extract module name\n",
        "                parts = line.split()\n",
        "                if len(parts) >= 2:\n",
        "                    module_name = parts[1].split('(')[0]\n",
        "            elif 'input' in line:\n",
        "                # Extract input signals\n",
        "                input_part = line.replace('input', '').replace(';', '').replace(',', '').strip()\n",
        "                if input_part:\n",
        "                    inputs.append(input_part)\n",
        "            elif 'output' in line:\n",
        "                # Extract output signals\n",
        "                output_part = line.replace('output', '').replace(';', '').replace(',', '').strip()\n",
        "                if output_part:\n",
        "                    outputs.append(output_part)\n",
        "\n",
        "        return {\n",
        "            \"module_name\": module_name,\n",
        "            \"inputs\": inputs,\n",
        "            \"outputs\": outputs\n",
        "        }\n",
        "\n",
        "    def _extract_section(self, text: str, marker: str, start_delim: str, end_delim: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract a section from the LLM response between delimiters.\n",
        "\n",
        "        Args:\n",
        "            text: Full response text\n",
        "            marker: Section marker to find\n",
        "            start_delim: Start delimiter (e.g., \"```verilog\")\n",
        "            end_delim: End delimiter (e.g., \"```\")\n",
        "\n",
        "        Returns:\n",
        "            Extracted section content\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Find the marker\n",
        "            marker_idx = text.find(marker)\n",
        "            if marker_idx == -1:\n",
        "                return \"\"\n",
        "\n",
        "            # Find the start delimiter after the marker\n",
        "            start_idx = text.find(start_delim, marker_idx)\n",
        "            if start_idx == -1:\n",
        "                return \"\"\n",
        "            start_idx += len(start_delim)\n",
        "\n",
        "            # Find the end delimiter\n",
        "            end_idx = text.find(end_delim, start_idx)\n",
        "            if end_idx == -1:\n",
        "                return \"\"\n",
        "\n",
        "            return text[start_idx:end_idx].strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting section: {e}\")\n",
        "            return \"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8ytSpYJVUb6"
      },
      "source": [
        "## Section 6: Golden Model Generator (Step 4)\n",
        "\n",
        "The `GoldenModelGenerator` class creates a Python reference implementation from the natural language description. It:\n",
        "- Converts the description into executable Python code\n",
        "- Runs all test patterns through the Python model\n",
        "- Captures expected outputs for verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GmW6ljEMVUb6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Step 4: Generate Python golden model and compute golden outputs.\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import io\n",
        "import json\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "\n",
        "class GoldenModelGenerator:\n",
        "    \"\"\"Generate Python golden model and compute expected outputs.\"\"\"\n",
        "\n",
        "    def __init__(self, llm_client: LLMClient):\n",
        "        \"\"\"\n",
        "        Initialize golden model generator.\n",
        "\n",
        "        Args:\n",
        "            llm_client: LLM client instance\n",
        "        \"\"\"\n",
        "        self.llm_client = llm_client\n",
        "\n",
        "    def generate_python_model(self, description: str, module_info: Dict[str, Any]) -> str:\n",
        "        \"\"\"\n",
        "        Generate Python implementation based on natural language description.\n",
        "\n",
        "        Args:\n",
        "            description: Natural language description of the module\n",
        "            module_info: Module information (name, inputs, outputs)\n",
        "\n",
        "        Returns:\n",
        "            Python code implementing the module functionality\n",
        "        \"\"\"\n",
        "        system_prompt = \"\"\"You are an expert in hardware design and Python programming.\n",
        "Your task is to create a Python function that implements the exact same functionality\n",
        "as described in the natural language specification.\"\"\"\n",
        "\n",
        "        user_prompt = f\"\"\"Given the following natural language description of a hardware module,\n",
        "create a Python function that implements this functionality.\n",
        "\n",
        "Natural Language Description:\n",
        "{description}\n",
        "\n",
        "Module Information:\n",
        "- Module Name: {module_info.get('module_name', 'unknown')}\n",
        "- Inputs: {module_info.get('inputs', [])}\n",
        "- Outputs: {module_info.get('outputs', [])}\n",
        "\n",
        "Create a Python function named '{module_info.get('module_name', 'module')}_golden' that:\n",
        "1. Takes the input signals as parameters\n",
        "2. Computes and returns the output signals\n",
        "3. Implements the exact functionality described\n",
        "4. Handles all edge cases properly\n",
        "5. Returns outputs as a dictionary with output signal names as keys\n",
        "\n",
        "Provide ONLY the Python function code, no explanations.\n",
        "Start with 'def {module_info.get('module_name', 'module')}_golden(' and include complete implementation.\n",
        "\"\"\"\n",
        "\n",
        "        response = self.llm_client.generate(user_prompt, system_prompt, temperature=0.2, max_tokens=3000)\n",
        "\n",
        "        # Extract Python code\n",
        "        python_code = self._extract_python_code(response)\n",
        "\n",
        "        return python_code\n",
        "\n",
        "    def compute_golden_outputs(self, python_code: str, test_patterns: List[Dict[str, Any]],\n",
        "                               module_info: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Execute the Python golden model with test patterns to get expected outputs.\n",
        "\n",
        "        Args:\n",
        "            python_code: Python golden model code\n",
        "            test_patterns: List of test input patterns\n",
        "            module_info: Module information\n",
        "\n",
        "        Returns:\n",
        "            List of test patterns with golden outputs added\n",
        "        \"\"\"\n",
        "        results = []\n",
        "\n",
        "        # Execute the Python code in a safe namespace\n",
        "        namespace = {}\n",
        "        try:\n",
        "            exec(python_code, namespace)\n",
        "        except Exception as e:\n",
        "            print(f\"Error executing Python code: {e}\")\n",
        "            return results\n",
        "\n",
        "        # Find the golden function\n",
        "        function_name = f\"{module_info.get('module_name', 'module')}_golden\"\n",
        "        golden_func = namespace.get(function_name)\n",
        "\n",
        "        if not golden_func:\n",
        "            print(f\"Error: Could not find function {function_name}\")\n",
        "            return results\n",
        "\n",
        "        # Run each test pattern through the golden model\n",
        "        for pattern in test_patterns:\n",
        "            try:\n",
        "                # Extract input values from the pattern\n",
        "                inputs = pattern.get('inputs', pattern)\n",
        "\n",
        "                # Call the golden function\n",
        "                if isinstance(inputs, dict):\n",
        "                    outputs = golden_func(**inputs)\n",
        "                else:\n",
        "                    # If inputs is not a dict, try to call with positional args\n",
        "                    outputs = golden_func(*inputs.values()) if hasattr(inputs, 'values') else golden_func(inputs)\n",
        "\n",
        "                # Add outputs to the pattern\n",
        "                result = pattern.copy()\n",
        "                result['expected_outputs'] = outputs\n",
        "                results.append(result)\n",
        "            except Exception as e:\n",
        "                print(f\"Error computing golden output for pattern {pattern}: {e}\")\n",
        "                result = pattern.copy()\n",
        "                result['expected_outputs'] = None\n",
        "                result['error'] = str(e)\n",
        "                results.append(result)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _extract_python_code(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract Python code from LLM response.\n",
        "\n",
        "        Args:\n",
        "            text: LLM response text\n",
        "\n",
        "        Returns:\n",
        "            Extracted Python code\n",
        "        \"\"\"\n",
        "        # Try to find code between ```python and ```\n",
        "        if \"```python\" in text:\n",
        "            start = text.find(\"```python\") + len(\"```python\")\n",
        "            end = text.find(\"```\", start)\n",
        "            if end != -1:\n",
        "                return text[start:end].strip()\n",
        "\n",
        "        # Try to find code between ``` and ```\n",
        "        if \"```\" in text:\n",
        "            parts = text.split(\"```\")\n",
        "            if len(parts) >= 3:\n",
        "                return parts[1].strip()\n",
        "\n",
        "        # If no code blocks found, look for def statement\n",
        "        if \"def \" in text:\n",
        "            lines = text.split('\\n')\n",
        "            code_lines = []\n",
        "            in_function = False\n",
        "            for line in lines:\n",
        "                if line.strip().startswith('def '):\n",
        "                    in_function = True\n",
        "                if in_function:\n",
        "                    code_lines.append(line)\n",
        "            return '\\n'.join(code_lines)\n",
        "\n",
        "        return text.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b-iwsAUVUb6"
      },
      "source": [
        "## Section 7: Testbench Updater (Step 5)\n",
        "\n",
        "The `TestbenchUpdater` class enhances the testbench with verification logic. It:\n",
        "- Injects expected outputs from the golden model\n",
        "- Adds pass/fail checking for each test case\n",
        "- Implements test summary reporting\n",
        "- Maintains proper Verilog formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_xaot9nhVUb6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Step 5: Update testbench with golden outputs.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, Any, List\n",
        "import re\n",
        "import json\n",
        "\n",
        "\n",
        "class TestbenchUpdater:\n",
        "    \"\"\"Update generated testbench with golden outputs using LLM.\"\"\"\n",
        "\n",
        "    def __init__(self, llm_client: LLMClient):\n",
        "        \"\"\"\n",
        "        Initialize testbench updater.\n",
        "\n",
        "        Args:\n",
        "            llm_client: LLM client instance\n",
        "        \"\"\"\n",
        "        self.llm_client = llm_client\n",
        "\n",
        "    def update_testbench(self, testbench_code: str, test_patterns_with_outputs: List[Dict[str, Any]],\n",
        "                        module_info: Dict[str, Any]) -> str:\n",
        "        \"\"\"\n",
        "        Update testbench code to include expected outputs and verification.\n",
        "\n",
        "        Args:\n",
        "            testbench_code: Original testbench code without expected outputs\n",
        "            test_patterns_with_outputs: Test patterns with golden outputs\n",
        "            module_info: Module information\n",
        "\n",
        "        Returns:\n",
        "            Updated testbench code with assertions and expected outputs\n",
        "        \"\"\"\n",
        "        # Use LLM if available, otherwise fall back to rule-based approach\n",
        "        if self.llm_client.is_available():\n",
        "            updated_code = self._llm_update_testbench(testbench_code, test_patterns_with_outputs, module_info)\n",
        "        else:\n",
        "            # Fallback to rule-based approach\n",
        "            updated_code = self._add_verification_logic(testbench_code, test_patterns_with_outputs, module_info)\n",
        "\n",
        "        return updated_code\n",
        "\n",
        "    def _llm_update_testbench(self, testbench_code: str, test_patterns_with_outputs: List[Dict[str, Any]],\n",
        "                             module_info: Dict[str, Any]) -> str:\n",
        "        \"\"\"\n",
        "        Use LLM to update testbench with verification logic.\n",
        "\n",
        "        Args:\n",
        "            testbench_code: Original testbench code\n",
        "            test_patterns_with_outputs: Test patterns with golden outputs\n",
        "            module_info: Module information\n",
        "\n",
        "        Returns:\n",
        "            Updated testbench code with verification logic\n",
        "        \"\"\"\n",
        "        system_prompt = \"\"\"You are an expert in Verilog testbench development and verification.\n",
        "Your task is to update a testbench by adding comprehensive verification logic and expected output checks.\n",
        "You should:\n",
        "1. Add verification for each test case using the provided expected outputs\n",
        "2. Track passed and failed test counts\n",
        "3. Display clear pass/fail messages for each output signal\n",
        "4. Generate a comprehensive test summary at the end\n",
        "5. Maintain the original testbench structure and style\n",
        "6. Use proper Verilog syntax and best practices\"\"\"\n",
        "\n",
        "        # Prepare test patterns data for the LLM\n",
        "        patterns_str = json.dumps(test_patterns_with_outputs, indent=2)\n",
        "\n",
        "        user_prompt = f\"\"\"Given the following Verilog testbench and test patterns with expected outputs,\n",
        "update the testbench to include verification logic that checks the actual outputs against the expected outputs.\n",
        "\n",
        "Original Testbench Code:\n",
        "```verilog\n",
        "{testbench_code}\n",
        "```\n",
        "\n",
        "Module Information:\n",
        "- Module Name: {module_info.get('module_name', 'unknown')}\n",
        "- Inputs: {module_info.get('inputs', [])}\n",
        "- Outputs: {module_info.get('outputs', [])}\n",
        "\n",
        "Test Patterns with Expected Outputs:\n",
        "```json\n",
        "{patterns_str}\n",
        "```\n",
        "\n",
        "Please update the testbench to:\n",
        "1. Add integer variables 'passed_tests' and 'failed_tests' at the beginning of the initial block (initialized to 0)\n",
        "2. After each test case (identified by $display statements), add a delay (#10) for outputs to settle\n",
        "3. For each output signal, compare the actual value against the expected value from the test patterns\n",
        "4. Display \"‚úì\" for passing checks and \"‚úó\" for failing checks with actual and expected values\n",
        "5. Increment passed_tests for each passing check and failed_tests for each failing check\n",
        "6. At the end of the initial block (before 'end'), add a test summary showing:\n",
        "   - Total tests run\n",
        "   - Number passed\n",
        "   - Number failed\n",
        "7. Preserve all original test case displays and structure\n",
        "8. Use proper indentation and formatting\n",
        "\n",
        "Provide ONLY the complete updated testbench code, no explanations.\n",
        "Format your response as:\n",
        "```verilog\n",
        "[updated testbench code here]\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "        response = self.llm_client.generate(user_prompt, system_prompt, max_tokens=8000)\n",
        "\n",
        "        # Extract the Verilog code from the response\n",
        "        updated_code = self._extract_verilog_code(response)\n",
        "\n",
        "        # If extraction failed, fall back to original with rule-based update\n",
        "        if not updated_code or len(updated_code) < len(testbench_code) // 2:\n",
        "            print(\"Warning: LLM response extraction failed, using rule-based approach\")\n",
        "            updated_code = self._add_verification_logic(testbench_code, test_patterns_with_outputs, module_info)\n",
        "\n",
        "        return updated_code\n",
        "\n",
        "    def _extract_verilog_code(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract Verilog code from LLM response.\n",
        "\n",
        "        Args:\n",
        "            text: LLM response text\n",
        "\n",
        "        Returns:\n",
        "            Extracted Verilog code\n",
        "        \"\"\"\n",
        "        # Try to find code between ```verilog and ```\n",
        "        if \"```verilog\" in text:\n",
        "            start = text.find(\"```verilog\") + len(\"```verilog\")\n",
        "            end = text.find(\"```\", start)\n",
        "            if end != -1:\n",
        "                return text[start:end].strip()\n",
        "\n",
        "        # Try to find code between ``` and ```\n",
        "        if \"```\" in text:\n",
        "            parts = text.split(\"```\")\n",
        "            if len(parts) >= 3:\n",
        "                # Get the first code block\n",
        "                code = parts[1].strip()\n",
        "                # If it starts with a language identifier, remove it\n",
        "                if code.startswith(\"verilog\\n\") or code.startswith(\"verilog \"):\n",
        "                    code = code.split('\\n', 1)[1] if '\\n' in code else code\n",
        "                return code.strip()\n",
        "\n",
        "        # If no code blocks found, look for module or testbench keywords\n",
        "        if \"module \" in text or \"initial \" in text:\n",
        "            return text.strip()\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    def _add_verification_logic(self, testbench_code: str, test_patterns: List[Dict[str, Any]],\n",
        "                               module_info: Dict[str, Any]) -> str:\n",
        "        \"\"\"\n",
        "        Add verification logic to the testbench.\n",
        "\n",
        "        Args:\n",
        "            testbench_code: Original testbench code\n",
        "            test_patterns: Test patterns with expected outputs\n",
        "            module_info: Module information\n",
        "\n",
        "        Returns:\n",
        "            Testbench code with verification logic added\n",
        "        \"\"\"\n",
        "        lines = testbench_code.split('\\n')\n",
        "        updated_lines = []\n",
        "\n",
        "        # Track if we're in the initial block\n",
        "        in_initial = False\n",
        "        indent_level = 0\n",
        "        test_case_num = 0\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            stripped = line.strip()\n",
        "\n",
        "            # Detect initial block\n",
        "            if 'initial' in stripped and 'begin' in stripped:\n",
        "                in_initial = True\n",
        "                updated_lines.append(line)\n",
        "                # Add test result tracking variables after initial begin\n",
        "                updated_lines.append(\"    integer passed_tests = 0;\")\n",
        "                updated_lines.append(\"    integer failed_tests = 0;\")\n",
        "                updated_lines.append(\"\")\n",
        "                continue\n",
        "\n",
        "            # Check for test case markers (e.g., $display for test cases)\n",
        "            if in_initial and '$display' in stripped and ('Test' in stripped or 'test' in stripped):\n",
        "                # This is likely a test case display\n",
        "                updated_lines.append(line)\n",
        "\n",
        "                # Add delay to let outputs settle\n",
        "                indent = len(line) - len(line.lstrip())\n",
        "                indent_str = ' ' * indent\n",
        "                updated_lines.append(f\"{indent_str}#10; // Wait for outputs to settle\")\n",
        "\n",
        "                # Add verification for this test case if we have expected outputs\n",
        "                if test_case_num < len(test_patterns):\n",
        "                    pattern = test_patterns[test_case_num]\n",
        "                    if 'expected_outputs' in pattern and pattern['expected_outputs']:\n",
        "                        verification_lines = self._generate_verification(\n",
        "                            pattern, module_info, indent\n",
        "                        )\n",
        "                        updated_lines.extend(verification_lines)\n",
        "                    test_case_num += 1\n",
        "                continue\n",
        "\n",
        "            # Check for end of initial block\n",
        "            if in_initial and (stripped == 'end' or stripped.startswith('end')):\n",
        "                # Add final summary before the end\n",
        "                indent = len(line) - len(line.lstrip())\n",
        "                indent_str = ' ' * indent\n",
        "                updated_lines.append(\"\")\n",
        "                updated_lines.append(f\"{indent_str}// Test Summary\")\n",
        "                updated_lines.append(f'{indent_str}$display(\"\\\\n========== Test Summary ==========\");')\n",
        "                updated_lines.append(f'{indent_str}$display(\"Total Tests: %0d\", passed_tests + failed_tests);')\n",
        "                updated_lines.append(f'{indent_str}$display(\"Passed: %0d\", passed_tests);')\n",
        "                updated_lines.append(f'{indent_str}$display(\"Failed: %0d\", failed_tests);')\n",
        "                updated_lines.append(f'{indent_str}$display(\"==================================\\\\n\");')\n",
        "                updated_lines.append(\"\")\n",
        "                updated_lines.append(line)\n",
        "                in_initial = False\n",
        "                continue\n",
        "\n",
        "            updated_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(updated_lines)\n",
        "\n",
        "    def _generate_verification(self, pattern: Dict[str, Any], module_info: Dict[str, Any],\n",
        "                              indent: int) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generate verification code for a single test pattern.\n",
        "\n",
        "        Args:\n",
        "            pattern: Test pattern with expected outputs\n",
        "            expected_outputs: Expected output values\n",
        "            module_info: Module information\n",
        "            indent: Indentation level\n",
        "\n",
        "        Returns:\n",
        "            List of verification code lines\n",
        "        \"\"\"\n",
        "        lines = []\n",
        "        indent_str = ' ' * indent\n",
        "\n",
        "        expected = pattern.get('expected_outputs', {})\n",
        "        if not expected:\n",
        "            return lines\n",
        "\n",
        "        # Get output signals\n",
        "        outputs = module_info.get('outputs', [])\n",
        "\n",
        "        # Generate verification for each output\n",
        "        for output in outputs:\n",
        "            output_name = output.split('[')[0].strip()  # Remove bit width if present\n",
        "            output_name = output_name.split()[-1]  # Get the signal name\n",
        "\n",
        "            if output_name in expected:\n",
        "                expected_value = expected[output_name]\n",
        "\n",
        "                # Check if expected value is boolean\n",
        "                if isinstance(expected_value, bool):\n",
        "                    expected_value = 1 if expected_value else 0\n",
        "\n",
        "                # Generate comparison\n",
        "                lines.append(f\"{indent_str}if ({output_name} === {expected_value}) begin\")\n",
        "                lines.append(f'{indent_str}    $display(\"  ‚úì {output_name} = %b (expected: {expected_value})\", {output_name});')\n",
        "                lines.append(f\"{indent_str}    passed_tests = passed_tests + 1;\")\n",
        "                lines.append(f\"{indent_str}end else begin\")\n",
        "                lines.append(f'{indent_str}    $display(\"  ‚úó {output_name} = %b (expected: {expected_value})\", {output_name});')\n",
        "                lines.append(f\"{indent_str}    failed_tests = failed_tests + 1;\")\n",
        "                lines.append(f\"{indent_str}end\")\n",
        "\n",
        "        return lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KZHBtPMVUb6"
      },
      "source": [
        "## Section 8: Testbench Generation Pipeline\n",
        "\n",
        "The `TestbenchPipeline` class coordinates all steps in the generation process. It:\n",
        "- Manages the flow from description to final testbench\n",
        "- Handles file I/O for all artifacts\n",
        "- Provides progress reporting\n",
        "- Implements graceful degradation when LLM is unavailable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GxrlFPIXVUb6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Main pipeline orchestrating the entire testbench generation process.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "from typing import Dict, Any, Optional\n",
        "import subprocess\n",
        "\n",
        "class TestbenchPipeline:\n",
        "    \"\"\"\n",
        "    Main pipeline for LLM-aided testbench generation.\n",
        "\n",
        "    Steps:\n",
        "    1. Accept natural language description and Verilog code\n",
        "    2. Generate testbench with test patterns (no golden outputs)\n",
        "    3. Generate Python golden model from description\n",
        "    4. Execute golden model with test patterns to get expected outputs\n",
        "    5. Update testbench with golden outputs and verification logic\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None, model: str = \"gpt-4\", provider: str = \"openai\"):\n",
        "        \"\"\"\n",
        "        Initialize the pipeline.\n",
        "\n",
        "        Args:\n",
        "            api_key: API key for LLM provider\n",
        "            model: Model name to use\n",
        "            provider: LLM provider name\n",
        "        \"\"\"\n",
        "        self.llm_client = LLMClient(api_key, model, provider)\n",
        "        self.testbench_gen = TestbenchGenerator(self.llm_client)\n",
        "        self.golden_gen = GoldenModelGenerator(self.llm_client)\n",
        "        self.testbench_updater = TestbenchUpdater(self.llm_client)\n",
        "\n",
        "    def run(self, description: str, verilog_code: str, output_dir: str = \"output\") -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Run the complete testbench generation pipeline.\n",
        "\n",
        "        Args:\n",
        "            description: Natural language description of the module\n",
        "            verilog_code: Verilog code to be tested\n",
        "            output_dir: Directory to save output files\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing all generated artifacts\n",
        "        \"\"\"\n",
        "        print(\"=\" * 80)\n",
        "        print(\"LLM-Aided Testbench Generation Pipeline\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Step 1 & 2: Input handling (description and verilog code are already provided)\n",
        "        print(\"\\n[Step 1-2] Input: Natural language description and Verilog code received\")\n",
        "        print(f\"Description length: {len(description)} characters\")\n",
        "        print(f\"Verilog code length: {len(verilog_code)} characters\")\n",
        "\n",
        "        # Step 3: Generate testbench with test patterns\n",
        "        print(\"\\n[Step 3] Generating testbench with test patterns...\")\n",
        "        if not self.llm_client.is_available():\n",
        "            print(\"WARNING: LLM client not configured. Using mock generation.\")\n",
        "            testbench_result = self._mock_testbench_generation(verilog_code)\n",
        "        else:\n",
        "            testbench_result = self.testbench_gen.generate_testbench(description, verilog_code)\n",
        "\n",
        "        print(f\"  - Generated testbench with {len(testbench_result['test_patterns'])} test patterns\")\n",
        "        print(f\"  - Module: {testbench_result['module_info']['module_name']}\")\n",
        "\n",
        "        # Save initial testbench (without golden outputs)\n",
        "        initial_tb_path = os.path.join(output_dir, \"testbench_initial.v\")\n",
        "        with open(initial_tb_path, 'w') as f:\n",
        "            f.write(testbench_result['testbench_code'])\n",
        "        print(f\"  - Saved initial testbench to: {initial_tb_path}\")\n",
        "\n",
        "        # Step 4: Generate Python golden model and compute golden outputs\n",
        "        print(\"\\n[Step 4] Generating Python golden model and computing expected outputs...\")\n",
        "        if not self.llm_client.is_available():\n",
        "            print(\"WARNING: LLM client not configured. Using mock golden model.\")\n",
        "            python_code = self._mock_python_model(testbench_result['module_info'])\n",
        "        else:\n",
        "            python_code = self.golden_gen.generate_python_model(\n",
        "                description,\n",
        "                testbench_result['module_info']\n",
        "            )\n",
        "\n",
        "        print(f\"  - Generated Python golden model ({len(python_code)} characters)\")\n",
        "\n",
        "        # change testbench_result['test_patterns'] value to integral values\n",
        "        patterns = []\n",
        "        for pattern in testbench_result['test_patterns']:\n",
        "            for key in pattern:\n",
        "                if isinstance(pattern[key], str):\n",
        "                    pattern[key] = int(pattern[key], 2)\n",
        "            patterns.append(pattern)\n",
        "        testbench_result['test_patterns'] = patterns\n",
        "\n",
        "        # Save Python golden model\n",
        "        python_path = os.path.join(output_dir, \"golden_model.py\")\n",
        "        with open(python_path, 'w') as f:\n",
        "            f.write(python_code)\n",
        "        print(f\"  - Saved Python golden model to: {python_path}\")\n",
        "\n",
        "        # Compute golden outputs\n",
        "        print(\"  - Computing golden outputs for all test patterns...\")\n",
        "        test_patterns_with_outputs = self.golden_gen.compute_golden_outputs(\n",
        "            python_code,\n",
        "            testbench_result['test_patterns'],\n",
        "            testbench_result['module_info']\n",
        "        )\n",
        "\n",
        "        successful_patterns = sum(1 for p in test_patterns_with_outputs\n",
        "                                 if 'expected_outputs' in p and p['expected_outputs'] is not None)\n",
        "        print(f\"  - Successfully computed outputs for {successful_patterns}/{len(test_patterns_with_outputs)} patterns\")\n",
        "\n",
        "        # Save test patterns with golden outputs\n",
        "        patterns_path = os.path.join(output_dir, \"test_patterns_with_golden.json\")\n",
        "        with open(patterns_path, 'w') as f:\n",
        "            json.dump(test_patterns_with_outputs, f, indent=2)\n",
        "        print(f\"  - Saved test patterns with golden outputs to: {patterns_path}\")\n",
        "\n",
        "        # Step 5: Update testbench with golden outputs\n",
        "        print(\"\\n[Step 5] Updating testbench with golden outputs and verification logic...\")\n",
        "        final_testbench = self.testbench_updater.update_testbench(\n",
        "            testbench_result['testbench_code'],\n",
        "            test_patterns_with_outputs,\n",
        "            testbench_result['module_info']\n",
        "        )\n",
        "\n",
        "        # Save final testbench\n",
        "        final_tb_path = os.path.join(output_dir, \"testbench_final.v\")\n",
        "        with open(final_tb_path, 'w') as f:\n",
        "            f.write(final_testbench)\n",
        "        print(f\"  - Saved final testbench to: {final_tb_path}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"Pipeline completed successfully!\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"\\nGenerated files in '{output_dir}':\")\n",
        "        print(f\"  - testbench_initial.v    : Initial testbench with test patterns\")\n",
        "        print(f\"  - golden_model.py        : Python reference implementation\")\n",
        "        print(f\"  - test_patterns_with_golden.json : Test patterns with expected outputs\")\n",
        "        print(f\"  - testbench_final.v      : Final testbench with verification\")\n",
        "        print()\n",
        "\n",
        "        return {\n",
        "            'description': description,\n",
        "            'verilog_code': verilog_code,\n",
        "            'module_info': testbench_result['module_info'],\n",
        "            'test_patterns': testbench_result['test_patterns'],\n",
        "            'initial_testbench': testbench_result['testbench_code'],\n",
        "            'python_golden_model': python_code,\n",
        "            'test_patterns_with_outputs': test_patterns_with_outputs,\n",
        "            'final_testbench': final_testbench,\n",
        "            'output_dir': output_dir\n",
        "        }\n",
        "\n",
        "    def _mock_testbench_generation(self, verilog_code: str) -> Dict[str, Any]:\n",
        "        \"\"\"Mock testbench generation when LLM is not available.\"\"\"\n",
        "        return {\n",
        "            'testbench_code': '// Mock testbench - LLM not configured\\n' + verilog_code,\n",
        "            'test_patterns': [],\n",
        "            'module_info': {\n",
        "                'module_name': 'unknown',\n",
        "                'inputs': [],\n",
        "                'outputs': []\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _mock_python_model(self, module_info: Dict[str, Any]) -> str:\n",
        "        \"\"\"Mock Python model generation when LLM is not available.\"\"\"\n",
        "        return f\"# Mock Python model - LLM not configured\\ndef {module_info['module_name']}_golden():\\n    pass\\n\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI7NwGTyVUb7"
      },
      "source": [
        "## Section 9: Example 1 - 2-to-1 Multiplexer\n",
        "\n",
        "Now let's run a complete example! We'll generate a testbench for a simple 2-to-1 multiplexer.\n",
        "\n",
        "### Natural Language Description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "104eTyidVUb7",
        "outputId": "36cb9c80-2d3b-4153-9ea7-26fbdfc16186"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natural Language Description:\n",
            "================================================================================\n",
            "\n",
            "A 2-to-1 multiplexer (MUX).\n",
            "\n",
            "The module takes two 1-bit input signals (a and b) and one 1-bit select signal (sel).\n",
            "\n",
            "Functionality:\n",
            "- Input 'a': First data input (1-bit)\n",
            "- Input 'b': Second data input (1-bit)\n",
            "- Input 'sel': Select signal (1-bit)\n",
            "- Output 'y': Selected output (1-bit)\n",
            "\n",
            "When sel is 0, the output y should be equal to input a.\n",
            "When sel is 1, the output y should be equal to input b.\n",
            "\n",
            "This is a combinational logic circuit with no state or memory.\n",
            "\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Natural language description of the module\n",
        "mux_description = \"\"\"\n",
        "A 2-to-1 multiplexer (MUX).\n",
        "\n",
        "The module takes two 1-bit input signals (a and b) and one 1-bit select signal (sel).\n",
        "\n",
        "Functionality:\n",
        "- Input 'a': First data input (1-bit)\n",
        "- Input 'b': Second data input (1-bit)\n",
        "- Input 'sel': Select signal (1-bit)\n",
        "- Output 'y': Selected output (1-bit)\n",
        "\n",
        "When sel is 0, the output y should be equal to input a.\n",
        "When sel is 1, the output y should be equal to input b.\n",
        "\n",
        "This is a combinational logic circuit with no state or memory.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"Natural Language Description:\")\n",
        "print(\"=\" * 80)\n",
        "print(mux_description)\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2fdYLskVUb7"
      },
      "source": [
        "### Verilog Module Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcF1MvhjVUb7",
        "outputId": "0b1f5ee9-e326-4e56-8fef-b2d36f3c3f82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verilog Module:\n",
            "================================================================================\n",
            "\n",
            "module mux2to1 (\n",
            "    input wire a,\n",
            "    input wire b,\n",
            "    input wire sel,\n",
            "    output wire y\n",
            ");\n",
            "    assign y = sel ? b : a;\n",
            "endmodule\n",
            "\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Verilog module to be tested\n",
        "mux_verilog_code = \"\"\"\n",
        "module mux2to1 (\n",
        "    input wire a,\n",
        "    input wire b,\n",
        "    input wire sel,\n",
        "    output wire y\n",
        ");\n",
        "    assign y = sel ? b : a;\n",
        "endmodule\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"Verilog Module:\")\n",
        "print(\"=\" * 80)\n",
        "print(mux_verilog_code)\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save the Verilog code for later simulation\n",
        "with open(f\"{output_dir}/mux2to1.v\", \"w\") as f:\n",
        "    f.write(mux_verilog_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7D0YdnlVUb7"
      },
      "source": [
        "### Run the Complete Pipeline\n",
        "\n",
        "Now we execute the 5-step pipeline to generate the testbench:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNliz5idVUb7",
        "outputId": "f4ee9567-cb62-46d1-c891-61879df90e76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Running LLM-Aided Testbench Generation Pipeline for MUX\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "LLM-Aided Testbench Generation Pipeline\n",
            "================================================================================\n",
            "\n",
            "[Step 1-2] Input: Natural language description and Verilog code received\n",
            "Description length: 462 characters\n",
            "Verilog code length: 134 characters\n",
            "\n",
            "[Step 3] Generating testbench with test patterns...\n",
            "  - Generated testbench with 8 test patterns\n",
            "  - Module: mux2to1\n",
            "  - Saved initial testbench to: LLM-aided-Testbench-Generation/examples/output/mux/testbench_initial.v\n",
            "\n",
            "[Step 4] Generating Python golden model and computing expected outputs...\n",
            "  - Generated Python golden model (79 characters)\n",
            "  - Saved Python golden model to: LLM-aided-Testbench-Generation/examples/output/mux/golden_model.py\n",
            "  - Computing golden outputs for all test patterns...\n",
            "  - Successfully computed outputs for 8/8 patterns\n",
            "  - Saved test patterns with golden outputs to: LLM-aided-Testbench-Generation/examples/output/mux/test_patterns_with_golden.json\n",
            "\n",
            "[Step 5] Updating testbench with golden outputs and verification logic...\n",
            "  - Saved final testbench to: LLM-aided-Testbench-Generation/examples/output/mux/testbench_final.v\n",
            "\n",
            "================================================================================\n",
            "Pipeline completed successfully!\n",
            "================================================================================\n",
            "\n",
            "Generated files in 'LLM-aided-Testbench-Generation/examples/output/mux':\n",
            "  - testbench_initial.v    : Initial testbench with test patterns\n",
            "  - golden_model.py        : Python reference implementation\n",
            "  - test_patterns_with_golden.json : Test patterns with expected outputs\n",
            "  - testbench_final.v      : Final testbench with verification\n",
            "\n",
            "\n",
            "‚úì MUX testbench generation completed successfully!\n",
            "\n",
            "Generated files are in: LLM-aided-Testbench-Generation/examples/output/mux/\n"
          ]
        }
      ],
      "source": [
        "# Initialize and run the pipeline\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Running LLM-Aided Testbench Generation Pipeline for MUX\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "# Create pipeline instance\n",
        "pipeline = TestbenchPipeline(\n",
        "    api_key=os.environ.get('OPENAI_API_KEY'),\n",
        "    model='gpt-4o',\n",
        "    provider='openai'\n",
        ")\n",
        "\n",
        "# Run the complete pipeline\n",
        "try:\n",
        "    result = pipeline.run(\n",
        "        description=mux_description,\n",
        "        verilog_code=mux_verilog_code,\n",
        "        output_dir=f\"{output_dir}/mux\"\n",
        "    )\n",
        "    print(\"\\n‚úì MUX testbench generation completed successfully!\")\n",
        "    print(f\"\\nGenerated files are in: {output_dir}/mux/\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚úó Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3-IH8VbVUb7"
      },
      "source": [
        "### View Generated Files\n",
        "\n",
        "Let's examine what was generated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ9rAcuMVUb7",
        "outputId": "8f84bd98-5fc6-49e0-8036-5801af08babe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Files:\n",
            "================================================================================\n",
            "  - golden_model.py                     (79 bytes)\n",
            "  - test_patterns_with_golden.json      (738 bytes)\n",
            "  - testbench_final.v                   (4,186 bytes)\n",
            "  - testbench_initial.v                 (1,535 bytes)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# List generated files\n",
        "import os\n",
        "\n",
        "print(\"Generated Files:\")\n",
        "print(\"=\" * 80)\n",
        "mux_output_dir = f\"{output_dir}/mux\"\n",
        "if os.path.exists(mux_output_dir):\n",
        "    for filename in sorted(os.listdir(mux_output_dir)):\n",
        "        filepath = os.path.join(mux_output_dir, filename)\n",
        "        if os.path.isfile(filepath):\n",
        "            size = os.path.getsize(filepath)\n",
        "            print(f\"  - {filename:35s} ({size:,} bytes)\")\n",
        "else:\n",
        "    print(\"  No files generated yet\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkpNYI3PVUb7"
      },
      "source": [
        "### Examine the Initial Testbench\n",
        "\n",
        "The initial testbench includes test patterns but not the expected outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f50Ow-INVUb7",
        "outputId": "f7d19f4c-02f1-4e04-ea0f-be160dd07293"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Testbench:\n",
            "================================================================================\n",
            "  1: module mux2to1_tb;\n",
            "  2: \n",
            "  3:     // Declare signals\n",
            "  4:     reg a, b, sel;\n",
            "  5:     wire y;\n",
            "  6: \n",
            "  7:     // Instantiate the module under test\n",
            "  8:     mux2to1 uut (\n",
            "  9:         .a(a),\n",
            " 10:         .b(b),\n",
            " 11:         .sel(sel),\n",
            " 12:         .y(y)\n",
            " 13:     );\n",
            " 14: \n",
            " 15:     initial begin\n",
            " 16:         // Test pattern 1: a=0, b=0, sel=0\n",
            " 17:         a = 0; b = 0; sel = 0;\n",
            " 18:         #10;\n",
            " 19:         $display(\"Test 1: a=%b, b=%b, sel=%b, y=%b\", a, b, sel, y);\n",
            " 20: \n",
            " 21:         // Test pattern 2: a=0, b=0, sel=1\n",
            " 22:         a = 0; b = 0; sel = 1;\n",
            " 23:         #10;\n",
            " 24:         $display(\"Test 2: a=%b, b=%b, sel=%b, y=%b\", a, b, sel, y);\n",
            " 25: \n",
            " 26:         // Test pattern 3: a=0, b=1, sel=0\n",
            " 27:         a = 0; b = 1; sel = 0;\n",
            " 28:         #10;\n",
            " 29:         $display(\"Test 3: a=%b, b=%b, sel=%b, y=%b\", a, b, sel, y);\n",
            " 30: \n",
            " 31:         // Test pattern 4: a=0, b=1, sel=1\n",
            " 32:         a = 0; b = 1; sel = 1;\n",
            " 33:         #10;\n",
            " 34:         $display(\"Test 4: a=%b, b=%b, sel=%b, y=%b\", a, b, sel, y);\n",
            " 35: \n",
            " 36:         // Test pattern 5: a=1, b=0, sel=0\n",
            " 37:         a = 1; b = 0; sel = 0;\n",
            " 38:         #10;\n",
            " 39:         $display(\"Test 5: a=%b, b=%b, sel=%b, y=%b\", a, b, sel, y);\n",
            " 40: \n",
            " 41:         // Test pattern 6: a=1, b=0, sel=1\n",
            " 42:         a = 1; b = 0; sel = 1;\n",
            " 43:         #10;\n",
            " 44:         $display(\"Test 6: a=%b, b=%b, sel=%b, y=%b\", a, b, sel, y);\n",
            " 45: \n",
            " 46:         // Test pattern 7: a=1, b=1, sel=0\n",
            " 47:         a = 1; b = 1; sel = 0;\n",
            " 48:         #10;\n",
            " 49:         $display(\"Test 7: a=%b, b=%b, sel=%b, y=%b\", a, b, sel, y);\n",
            " 50: \n",
            " 51:         // Test pattern 8: a=1, b=1, sel=1\n",
            " 52:         a = 1; b = 1; sel = 1;\n",
            " 53:         #10;\n",
            " 54:         $display(\"Test 8: a=%b, b=%b, sel=%b, y=%b\", a, b, sel, y);\n",
            " 55: \n",
            " 56:         // Finish simulation\n",
            " 57:         $finish;\n",
            " 58:     end\n",
            " 59: \n",
            " 60: endmodule================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Display initial testbench (first 50 lines)\n",
        "try:\n",
        "    with open(f\"{output_dir}/mux/testbench_initial.v\", \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        print(\"Initial Testbench:\")\n",
        "        print(\"=\" * 80)\n",
        "        for i, line in enumerate(lines, 1):\n",
        "            print(f\"{i:3d}: {line}\", end=\"\")\n",
        "        print(\"=\" * 80)\n",
        "except FileNotFoundError:\n",
        "    print(\"Initial testbench file not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOoMbDB9VUb7"
      },
      "source": [
        "### Examine the Python Golden Model\n",
        "\n",
        "The golden model implements the expected behavior in Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfPlRbR9VUb7",
        "outputId": "003642a5-141b-4dbc-9de3-d456d6950aa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python Golden Model:\n",
            "================================================================================\n",
            "def mux2to1_golden(a, b, sel):\n",
            "    y = a if sel == 0 else b\n",
            "    return {'y': y}\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Display Python golden model\n",
        "try:\n",
        "    with open(f\"{output_dir}/mux/golden_model.py\", \"r\") as f:\n",
        "        golden_code = f.read()\n",
        "        print(\"Python Golden Model:\")\n",
        "        print(\"=\" * 80)\n",
        "        print(golden_code)\n",
        "        print(\"=\" * 80)\n",
        "except FileNotFoundError:\n",
        "    print(\"Golden model file not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mBZJPsUVUb7"
      },
      "source": [
        "### Examine Test Patterns with Expected Outputs\n",
        "\n",
        "The test patterns JSON file contains all test cases with their expected outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hbck12FVUb7",
        "outputId": "dd277078-d15d-410f-b4f7-896f99d6b24c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Patterns with Golden Outputs (8 patterns):\n",
            "================================================================================\n",
            "\n",
            "Pattern 1:\n",
            "  Inputs:  {'a': 0, 'b': 0, 'sel': 0, 'expected_outputs': {'y': 0}}\n",
            "  Expected: {'y': 0}\n",
            "\n",
            "Pattern 2:\n",
            "  Inputs:  {'a': 0, 'b': 0, 'sel': 1, 'expected_outputs': {'y': 0}}\n",
            "  Expected: {'y': 0}\n",
            "\n",
            "Pattern 3:\n",
            "  Inputs:  {'a': 0, 'b': 1, 'sel': 0, 'expected_outputs': {'y': 0}}\n",
            "  Expected: {'y': 0}\n",
            "\n",
            "Pattern 4:\n",
            "  Inputs:  {'a': 0, 'b': 1, 'sel': 1, 'expected_outputs': {'y': 1}}\n",
            "  Expected: {'y': 1}\n",
            "\n",
            "Pattern 5:\n",
            "  Inputs:  {'a': 1, 'b': 0, 'sel': 0, 'expected_outputs': {'y': 1}}\n",
            "  Expected: {'y': 1}\n",
            "\n",
            "Pattern 6:\n",
            "  Inputs:  {'a': 1, 'b': 0, 'sel': 1, 'expected_outputs': {'y': 0}}\n",
            "  Expected: {'y': 0}\n",
            "\n",
            "Pattern 7:\n",
            "  Inputs:  {'a': 1, 'b': 1, 'sel': 0, 'expected_outputs': {'y': 1}}\n",
            "  Expected: {'y': 1}\n",
            "\n",
            "Pattern 8:\n",
            "  Inputs:  {'a': 1, 'b': 1, 'sel': 1, 'expected_outputs': {'y': 1}}\n",
            "  Expected: {'y': 1}\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Display test patterns with golden outputs\n",
        "try:\n",
        "    with open(f\"{output_dir}/mux/test_patterns_with_golden.json\", \"r\") as f:\n",
        "        patterns = json.load(f)\n",
        "        print(f\"Test Patterns with Golden Outputs ({len(patterns)} patterns):\")\n",
        "        print(\"=\" * 80)\n",
        "        for i, pattern in enumerate(patterns, 1):\n",
        "            print(f\"\\nPattern {i}:\")\n",
        "            print(f\"  Inputs:  {pattern.get('inputs', pattern)}\")\n",
        "            print(f\"  Expected: {pattern.get('expected_outputs', 'N/A')}\")\n",
        "        print(\"=\" * 80)\n",
        "except FileNotFoundError:\n",
        "    print(\"Test patterns file not found\")\n",
        "except json.JSONDecodeError:\n",
        "    print(\"Error decoding test patterns JSON\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65ND4fJcVUb7"
      },
      "source": [
        "### Examine the Final Testbench with Verification\n",
        "\n",
        "The final testbench includes all verification logic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSLMk5HeVUb7",
        "outputId": "9644dafa-4b75-475a-b159-3aa0f7d65334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Testbench with Verification (134 lines total):\n",
            "================================================================================\n",
            "  1: module mux2to1_tb;\n",
            "  2: \n",
            "  3:     // Declare signals\n",
            "  4:     reg a, b, sel;\n",
            "  5:     wire y;\n",
            "  6: \n",
            "  7:     // Instantiate the module under test\n",
            "  8:     mux2to1 uut (\n",
            "  9:         .a(a),\n",
            " 10:         .b(b),\n",
            " 11:         .sel(sel),\n",
            " 12:         .y(y)\n",
            " 13:     );\n",
            " 14: \n",
            " 15:     integer passed_tests;\n",
            " 16:     integer failed_tests;\n",
            " 17: \n",
            " 18:     initial begin\n",
            " 19:         passed_tests = 0;\n",
            " 20:         failed_tests = 0;\n",
            " 21: \n",
            " 22:         // Test pattern 1: a=0, b=0, sel=0\n",
            " 23:         a = 0; b = 0; sel = 0;\n",
            " 24:         #10;\n",
            " 25:         $display(\"Test 1: a=%b, b=%b, sel=%b, y=%b\", a, b, sel, y);\n",
            " 26:         #10;\n",
            " 27:         if (y === 0) begin\n",
            " 28:             $display(\"‚úì Test 1 Passed: y=%b (Expected: 0)\", y);\n",
            " 29:             passed_tests = passed_tests + 1;\n",
            " 30:         end else begin\n",
            " 31:             $display(\"‚úó Test 1 Failed: y=%b (Expected: 0)\", y);\n",
            " 32:             failed_tests = failed_tests + 1;\n",
            " 33:         end\n",
            " 34: \n",
            " 35:         // Test pattern 2: a=0, b=0, sel=1\n",
            " 36:         a = 0; b = 0; sel = 1;\n",
            " 37:         #10;\n",
            " 38:         $display(\"Test 2: a=%b, b=%b, sel=%b, y=%b\", a, b, sel, y);\n",
            " 39:         #10;\n",
            " 40:         if (y === 0) begin\n",
            " 41:             $display(\"‚úì Test 2 Passed: y=%b (Expected: 0)\", y);\n",
            " 42:             passed_tests = passed_tests + 1;\n",
            " 43:         end else begin\n",
            " 44:             $display(\"‚úó Test 2 Failed: y=%b (Expected: 0)\", y);\n",
            " 45:             failed_tests = failed_tests + 1;\n",
            " 46:         end\n",
            " 47: \n",
            " 48:         // Test pattern 3: a=0, b=1, sel=0\n",
            " 49:         a = 0; b = 1; sel = 0;\n",
            " 50:         #10;\n",
            " 51:         $display(\"Test 3: a=%b, b=%b, sel=%b, y=%b\", a, b, sel, y);\n",
            " 52:         #10;\n",
            " 53:         if (y === 0) begin\n",
            " 54:             $display(\"‚úì Test 3 Passed: y=%b (Expected: 0)\", y);\n",
            " 55:             passed_tests = passed_tests + 1;\n",
            " 56:         end else begin\n",
            " 57:             $display(\"‚úó Test 3 Failed: y=%b (Expected: 0)\", y);\n",
            " 58:             failed_tests = failed_tests + 1;\n",
            " 59:         end\n",
            " 60: \n",
            " 61:         // Test pattern 4: a=0, b=1, sel=1\n",
            " 62:         a = 0; b = 1; sel = 1;\n",
            " 63:         #10;\n",
            " 64:         $display(\"Test 4: a=%b, b=%b, sel=%b, y=%b\", a, b, sel, y);\n",
            " 65:         #10;\n",
            " 66:         if (y === 1) begin\n",
            " 67:             $display(\"‚úì Test 4 Passed: y=%b (Expected: 1)\", y);\n",
            " 68:             passed_tests = passed_tests + 1;\n",
            " 69:         end else begin\n",
            " 70:             $display(\"‚úó Test 4 Failed: y=%b (Expected: 1)\", y);\n",
            " 71:             failed_tests = failed_tests + 1;\n",
            " 72:         end\n",
            " 73: \n",
            " 74:         // Test pattern 5: a=1, b=0, sel=0\n",
            " 75:         a = 1; b = 0; sel = 0;\n",
            " 76:         #10;\n",
            " 77:         $display(\"Test 5: a=%b, b=%b, sel=%b, y=%b\", a, b, sel, y);\n",
            " 78:         #10;\n",
            " 79:         if (y === 1) begin\n",
            " 80:             $display(\"‚úì Test 5 Passed: y=%b (Expected: 1)\", y);\n",
            " 81:             passed_tests = passed_tests + 1;\n",
            " 82:         end else begin\n",
            " 83:             $display(\"‚úó Test 5 Failed: y=%b (Expected: 1)\", y);\n",
            " 84:             failed_tests = failed_tests + 1;\n",
            " 85:         end\n",
            " 86: \n",
            " 87:         // Test pattern 6: a=1, b=0, sel=1\n",
            " 88:         a = 1; b = 0; sel = 1;\n",
            " 89:         #10;\n",
            " 90:         $display(\"Test 6: a=%b, b=%b, sel=%b, y=%b\", a, b, sel, y);\n",
            " 91:         #10;\n",
            " 92:         if (y === 0) begin\n",
            " 93:             $display(\"‚úì Test 6 Passed: y=%b (Expected: 0)\", y);\n",
            " 94:             passed_tests = passed_tests + 1;\n",
            " 95:         end else begin\n",
            " 96:             $display(\"‚úó Test 6 Failed: y=%b (Expected: 0)\", y);\n",
            " 97:             failed_tests = failed_tests + 1;\n",
            " 98:         end\n",
            " 99: \n",
            "100:         // Test pattern 7: a=1, b=1, sel=0\n",
            "101:         a = 1; b = 1; sel = 0;\n",
            "102:         #10;\n",
            "103:         $display(\"Test 7: a=%b, b=%b, sel=%b, y=%b\", a, b, sel, y);\n",
            "104:         #10;\n",
            "105:         if (y === 1) begin\n",
            "106:             $display(\"‚úì Test 7 Passed: y=%b (Expected: 1)\", y);\n",
            "107:             passed_tests = passed_tests + 1;\n",
            "108:         end else begin\n",
            "109:             $display(\"‚úó Test 7 Failed: y=%b (Expected: 1)\", y);\n",
            "110:             failed_tests = failed_tests + 1;\n",
            "111:         end\n",
            "112: \n",
            "113:         // Test pattern 8: a=1, b=1, sel=1\n",
            "114:         a = 1; b = 1; sel = 1;\n",
            "115:         #10;\n",
            "116:         $display(\"Test 8: a=%b, b=%b, sel=%b, y=%b\", a, b, sel, y);\n",
            "117:         #10;\n",
            "118:         if (y === 1) begin\n",
            "119:             $display(\"‚úì Test 8 Passed: y=%b (Expected: 1)\", y);\n",
            "120:             passed_tests = passed_tests + 1;\n",
            "121:         end else begin\n",
            "122:             $display(\"‚úó Test 8 Failed: y=%b (Expected: 1)\", y);\n",
            "123:             failed_tests = failed_tests + 1;\n",
            "124:         end\n",
            "125: \n",
            "126:         // Test summary\n",
            "127:         $display(\"Test Summary: Total=%0d, Passed=%0d, Failed=%0d\", \n",
            "128:                  passed_tests + failed_tests, passed_tests, failed_tests);\n",
            "129: \n",
            "130:         // Finish simulation\n",
            "131:         $finish;\n",
            "132:     end\n",
            "133: \n",
            "134: endmodule================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Display final testbench (first 60 lines)\n",
        "try:\n",
        "    with open(f\"{output_dir}/mux/testbench_final.v\", \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        print(f\"Final Testbench with Verification ({len(lines)} lines total):\")\n",
        "        print(\"=\" * 80)\n",
        "        for i, line in enumerate(lines, 1):\n",
        "            print(f\"{i:3d}: {line}\", end=\"\")\n",
        "        print(\"=\" * 80)\n",
        "except FileNotFoundError:\n",
        "    print(\"Final testbench file not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wInluRiyVUb7"
      },
      "source": [
        "### Simulate the Testbench\n",
        "\n",
        "If you have Icarus Verilog installed, we can simulate the testbench to see the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeSl7SSXVUb7",
        "outputId": "87818900-c1f3-467e-9cb3-2cb002ca60bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compiling testbench...\n",
            "‚úì Compilation successful\n",
            "\n",
            "Running simulation...\n",
            "Simulation Output:\n",
            "================================================================================\n",
            "Test 1: a=0, b=0, sel=0, y=0\n",
            "‚úì Test 1 Passed: y=0 (Expected: 0)\n",
            "Test 2: a=0, b=0, sel=1, y=0\n",
            "‚úì Test 2 Passed: y=0 (Expected: 0)\n",
            "Test 3: a=0, b=1, sel=0, y=0\n",
            "‚úì Test 3 Passed: y=0 (Expected: 0)\n",
            "Test 4: a=0, b=1, sel=1, y=1\n",
            "‚úì Test 4 Passed: y=1 (Expected: 1)\n",
            "Test 5: a=1, b=0, sel=0, y=1\n",
            "‚úì Test 5 Passed: y=1 (Expected: 1)\n",
            "Test 6: a=1, b=0, sel=1, y=0\n",
            "‚úì Test 6 Passed: y=0 (Expected: 0)\n",
            "Test 7: a=1, b=1, sel=0, y=1\n",
            "‚úì Test 7 Passed: y=1 (Expected: 1)\n",
            "Test 8: a=1, b=1, sel=1, y=1\n",
            "‚úì Test 8 Passed: y=1 (Expected: 1)\n",
            "Test Summary: Total=8, Passed=8, Failed=0\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Simulate the testbench with Icarus Verilog\n",
        "try:\n",
        "    # Compile\n",
        "    print(\"Compiling testbench...\")\n",
        "    compile_result = subprocess.run(\n",
        "        f\"iverilog -g2012 -o {output_dir}/mux/sim.vvp {output_dir}/mux2to1.v {output_dir}/mux/testbench_final.v\",\n",
        "        shell=True,\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=30\n",
        "    )\n",
        "\n",
        "    if compile_result.returncode != 0:\n",
        "        print(f\"Compilation failed:\")\n",
        "        print(compile_result.stderr)\n",
        "    else:\n",
        "        print(\"‚úì Compilation successful\\n\")\n",
        "\n",
        "        # Run simulation\n",
        "        print(\"Running simulation...\")\n",
        "        sim_result = subprocess.run(\n",
        "            f\"vvp {output_dir}/mux/sim.vvp\",\n",
        "            shell=True,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=30\n",
        "        )\n",
        "\n",
        "        print(\"Simulation Output:\")\n",
        "        print(\"=\" * 80)\n",
        "        print(sim_result.stdout)\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        if sim_result.stderr:\n",
        "            print(\"Warnings/Errors:\")\n",
        "            print(sim_result.stderr)\n",
        "\n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"Simulation timed out\")\n",
        "except FileNotFoundError:\n",
        "    print(\"iverilog not found. Please install Icarus Verilog to run simulations.\")\n",
        "    print(\"On Ubuntu/Debian: sudo apt-get install iverilog\")\n",
        "    print(\"On macOS: brew install icarus-verilog\")\n",
        "except Exception as e:\n",
        "    print(f\"Simulation error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d382HWyVUb7"
      },
      "source": [
        "## Section 10: Example 2 - 4-bit Adder\n",
        "\n",
        "Let's try another example with a more complex module - a 4-bit adder with carry output.\n",
        "\n",
        "### Natural Language Description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLX3Q865VUb7",
        "outputId": "46c2f587-e323-44af-8adc-773d8c274564"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natural Language Description:\n",
            "================================================================================\n",
            "\n",
            "A simple 4-bit adder module.\n",
            "\n",
            "The module takes two 4-bit input signals (a and b) and produces a 4-bit sum output and a 1-bit carry output.\n",
            "\n",
            "Functionality:\n",
            "- Input 'a': 4-bit unsigned number\n",
            "- Input 'b': 4-bit unsigned number  \n",
            "- Output 'sum': 4-bit result of a + b (lower 4 bits)\n",
            "- Output 'carry': 1-bit carry-out flag (set to 1 if result exceeds 15)\n",
            "\n",
            "The adder performs unsigned addition of the two 4-bit inputs.\n",
            "If the result is greater than 15 (0xF), the carry output should be set to 1.\n",
            "\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Natural language description of the 4-bit adder\n",
        "adder_description = \"\"\"\n",
        "A simple 4-bit adder module.\n",
        "\n",
        "The module takes two 4-bit input signals (a and b) and produces a 4-bit sum output and a 1-bit carry output.\n",
        "\n",
        "Functionality:\n",
        "- Input 'a': 4-bit unsigned number\n",
        "- Input 'b': 4-bit unsigned number\n",
        "- Output 'sum': 4-bit result of a + b (lower 4 bits)\n",
        "- Output 'carry': 1-bit carry-out flag (set to 1 if result exceeds 15)\n",
        "\n",
        "The adder performs unsigned addition of the two 4-bit inputs.\n",
        "If the result is greater than 15 (0xF), the carry output should be set to 1.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"Natural Language Description:\")\n",
        "print(\"=\" * 80)\n",
        "print(adder_description)\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kS9tLcJVUb7"
      },
      "source": [
        "### Verilog Module Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiU4vegZVUb7",
        "outputId": "1ccf33ec-1c28-4fa0-b601-763e68239abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verilog Module:\n",
            "================================================================================\n",
            "\n",
            "module adder4bit (\n",
            "    input wire [3:0] a,\n",
            "    input wire [3:0] b,\n",
            "    output wire [3:0] sum,\n",
            "    output wire carry\n",
            ");\n",
            "    wire [4:0] result;\n",
            "    assign result = a + b;\n",
            "    assign sum = result[3:0];\n",
            "    assign carry = result[4];\n",
            "endmodule\n",
            "\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Verilog module to be tested\n",
        "adder_verilog_code = \"\"\"\n",
        "module adder4bit (\n",
        "    input wire [3:0] a,\n",
        "    input wire [3:0] b,\n",
        "    output wire [3:0] sum,\n",
        "    output wire carry\n",
        ");\n",
        "    wire [4:0] result;\n",
        "    assign result = a + b;\n",
        "    assign sum = result[3:0];\n",
        "    assign carry = result[4];\n",
        "endmodule\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"Verilog Module:\")\n",
        "print(\"=\" * 80)\n",
        "print(adder_verilog_code)\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save the Verilog code\n",
        "with open(f\"{output_dir}/adder4bit.v\", \"w\") as f:\n",
        "    f.write(adder_verilog_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bULifVetVUb-"
      },
      "source": [
        "### Run the Pipeline for Adder\n",
        "\n",
        "Execute the complete pipeline for the 4-bit adder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKVVrIZZVUb-",
        "outputId": "567b9e81-b84f-4943-8340-aa272b382545"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Running LLM-Aided Testbench Generation Pipeline for 4-bit Adder\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "LLM-Aided Testbench Generation Pipeline\n",
            "================================================================================\n",
            "\n",
            "[Step 1-2] Input: Natural language description and Verilog code received\n",
            "Description length: 493 characters\n",
            "Verilog code length: 241 characters\n",
            "\n",
            "[Step 3] Generating testbench with test patterns...\n",
            "  - Generated testbench with 10 test patterns\n",
            "  - Module: adder4bit\n",
            "  - Saved initial testbench to: LLM-aided-Testbench-Generation/examples/output/adder/testbench_initial.v\n",
            "\n",
            "[Step 4] Generating Python golden model and computing expected outputs...\n",
            "  - Generated Python golden model (359 characters)\n",
            "  - Saved Python golden model to: LLM-aided-Testbench-Generation/examples/output/adder/golden_model.py\n",
            "  - Computing golden outputs for all test patterns...\n",
            "  - Successfully computed outputs for 10/10 patterns\n",
            "  - Saved test patterns with golden outputs to: LLM-aided-Testbench-Generation/examples/output/adder/test_patterns_with_golden.json\n",
            "\n",
            "[Step 5] Updating testbench with golden outputs and verification logic...\n",
            "  - Saved final testbench to: LLM-aided-Testbench-Generation/examples/output/adder/testbench_final.v\n",
            "\n",
            "================================================================================\n",
            "Pipeline completed successfully!\n",
            "================================================================================\n",
            "\n",
            "Generated files in 'LLM-aided-Testbench-Generation/examples/output/adder':\n",
            "  - testbench_initial.v    : Initial testbench with test patterns\n",
            "  - golden_model.py        : Python reference implementation\n",
            "  - test_patterns_with_golden.json : Test patterns with expected outputs\n",
            "  - testbench_final.v      : Final testbench with verification\n",
            "\n",
            "\n",
            "‚úì Adder testbench generation completed successfully!\n",
            "\n",
            "Generated files are in: LLM-aided-Testbench-Generation/examples/output/adder/\n"
          ]
        }
      ],
      "source": [
        "# Run pipeline for the adder\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Running LLM-Aided Testbench Generation Pipeline for 4-bit Adder\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    result = pipeline.run(\n",
        "        description=adder_description,\n",
        "        verilog_code=adder_verilog_code,\n",
        "        output_dir=f\"{output_dir}/adder\"\n",
        "    )\n",
        "    print(\"\\n‚úì Adder testbench generation completed successfully!\")\n",
        "    print(f\"\\nGenerated files are in: {output_dir}/adder/\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚úó Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-srPY3RVUb_"
      },
      "source": [
        "### Simulate the Adder Testbench\n",
        "\n",
        "Run the simulation for the adder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpYgySzZVUb_",
        "outputId": "11435a6f-e12d-4bd9-b9fa-3a1c902e9da4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compiling adder testbench...\n",
            "‚úì Compilation successful\n",
            "\n",
            "Running simulation...\n",
            "Simulation Output:\n",
            "================================================================================\n",
            "Test 1: a = 0000, b = 0000\n",
            "Sum: 0000 (‚úì), Carry: 0 (‚úì)\n",
            "Test 2: a = 1111, b = 0000\n",
            "Sum: 1111 (‚úì), Carry: 0 (‚úì)\n",
            "Test 3: a = 0000, b = 1111\n",
            "Sum: 1111 (‚úì), Carry: 0 (‚úì)\n",
            "Test 4: a = 1111, b = 1111\n",
            "Sum: 1110 (‚úì), Carry: 1 (‚úì)\n",
            "Test 5: a = 0010, b = 0011\n",
            "Sum: 0101 (‚úì), Carry: 0 (‚úì)\n",
            "Test 6: a = 0111, b = 0111\n",
            "Sum: 1110 (‚úì), Carry: 0 (‚úì)\n",
            "Test 7: a = 1000, b = 0111\n",
            "Sum: 1111 (‚úì), Carry: 0 (‚úì)\n",
            "Test 8: a = 1000, b = 1000\n",
            "Sum: 0000 (‚úì), Carry: 1 (‚úì)\n",
            "Test 9: a = 0101, b = 1010\n",
            "Sum: 1111 (‚úì), Carry: 0 (‚úì)\n",
            "Test 10: a = 1100, b = 0011\n",
            "Sum: 1111 (‚úì), Carry: 0 (‚úì)\n",
            "Test Summary:\n",
            "Total Tests Run:          20\n",
            "Tests Passed:          20\n",
            "Tests Failed:           0\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Simulate the adder testbench\n",
        "try:\n",
        "    # Compile\n",
        "    print(\"Compiling adder testbench...\")\n",
        "    compile_result = subprocess.run(\n",
        "        f\"iverilog -g2012 -o {output_dir}/adder/sim.vvp {output_dir}/adder4bit.v {output_dir}/adder/testbench_final.v\",\n",
        "        shell=True,\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=30\n",
        "    )\n",
        "\n",
        "    if compile_result.returncode != 0:\n",
        "        print(f\"Compilation failed:\")\n",
        "        print(compile_result.stderr)\n",
        "    else:\n",
        "        print(\"‚úì Compilation successful\\n\")\n",
        "\n",
        "        # Run simulation\n",
        "        print(\"Running simulation...\")\n",
        "        sim_result = subprocess.run(\n",
        "            f\"vvp {output_dir}/adder/sim.vvp\",\n",
        "            shell=True,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=30\n",
        "        )\n",
        "\n",
        "        print(\"Simulation Output:\")\n",
        "        print(\"=\" * 80)\n",
        "        print(sim_result.stdout)\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"iverilog not found. Skipping simulation.\")\n",
        "except Exception as e:\n",
        "    print(f\"Simulation error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS2npMOzcD6y"
      },
      "source": [
        "## Section 11: Example binary_to_bcd\n",
        "\n",
        "\n",
        "### Natural Language Description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qpR18fScCqk",
        "outputId": "e8032607-5abc-48fd-9f41-40f7da403cc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natural Language Description:\n",
            "================================================================================\n",
            "\n",
            "I am trying to create a Verilog model binary_to_bcd_converter for a binary to binary-coded-decimal converter. It must meet the following specifications:\n",
            "\t- Inputs:\n",
            "\t\t- binary_input (5-bits)\n",
            "\t- Outputs:\n",
            "\t\t- bcd_output (8-bits: 4-bits for the 10's place and 4-bits for the 1's place)\n",
            "\n",
            "How would I write a design that meets these specifications?\n",
            "\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Natural language description of the 4-bit adder\n",
        "binary_to_bcd_description = \"\"\"\n",
        "I am trying to create a Verilog model binary_to_bcd_converter for a binary to binary-coded-decimal converter. It must meet the following specifications:\n",
        "\t- Inputs:\n",
        "\t\t- binary_input (5-bits)\n",
        "\t- Outputs:\n",
        "\t\t- bcd_output (8-bits: 4-bits for the 10's place and 4-bits for the 1's place)\n",
        "\n",
        "How would I write a design that meets these specifications?\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"Natural Language Description:\")\n",
        "print(\"=\" * 80)\n",
        "print(binary_to_bcd_description)\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2KOovmWc5ZY"
      },
      "source": [
        "### Verilog Module Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY7R82Mbczma",
        "outputId": "6c30f935-894b-4ac0-cfc9-b4aa20997db8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verilog Module:\n",
            "================================================================================\n",
            "\n",
            "module binary_to_bcd_converter (\n",
            "    input [4:0] binary_input,\n",
            "    output reg [7:0] bcd_output\n",
            ");\n",
            "    integer i;\n",
            "    reg [3:0] tens;\n",
            "    reg [3:0] ones;\n",
            "    reg [4:0] temp;\n",
            "\n",
            "    always @* begin\n",
            "        // Initialize BCD output to zero\n",
            "        bcd_output = 8'b0;\n",
            "        tens = 4'b0;\n",
            "        ones = 4'b0;\n",
            "        temp = binary_input;\n",
            "\n",
            "        // Convert binary to BCD using the Double Dabble algorithm\n",
            "        for (i = 0; i < 5; i = i + 1) begin\n",
            "            if (tens >= 4'd5) begin\n",
            "                tens = tens + 4'd3;\n",
            "            end\n",
            "            if (ones >= 4'd5) begin\n",
            "                ones = ones + 4'd3;\n",
            "            end\n",
            "\n",
            "            // Shift left\n",
            "            tens = {tens[2:0], ones[3]};\n",
            "            ones = {ones[2:0], temp[4]};\n",
            "            temp = temp << 1;\n",
            "        end\n",
            "\n",
            "        // Assign the BCD output\n",
            "        bcd_output = {tens, ones};\n",
            "    end\n",
            "endmodule\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Verilog module to be tested\n",
        "binary_to_bcd_verilog_code = \"\"\"\n",
        "module binary_to_bcd_converter (\n",
        "    input [4:0] binary_input,\n",
        "    output reg [7:0] bcd_output\n",
        ");\n",
        "    integer i;\n",
        "    reg [3:0] tens;\n",
        "    reg [3:0] ones;\n",
        "    reg [4:0] temp;\n",
        "\n",
        "    always @* begin\n",
        "        // Initialize BCD output to zero\n",
        "        bcd_output = 8'b0;\n",
        "        tens = 4'b0;\n",
        "        ones = 4'b0;\n",
        "        temp = binary_input;\n",
        "\n",
        "        // Convert binary to BCD using the Double Dabble algorithm\n",
        "        for (i = 0; i < 5; i = i + 1) begin\n",
        "            if (tens >= 4'd5) begin\n",
        "                tens = tens + 4'd3;\n",
        "            end\n",
        "            if (ones >= 4'd5) begin\n",
        "                ones = ones + 4'd3;\n",
        "            end\n",
        "\n",
        "            // Shift left\n",
        "            tens = {tens[2:0], ones[3]};\n",
        "            ones = {ones[2:0], temp[4]};\n",
        "            temp = temp << 1;\n",
        "        end\n",
        "\n",
        "        // Assign the BCD output\n",
        "        bcd_output = {tens, ones};\n",
        "    end\n",
        "endmodule\n",
        "\"\"\"\n",
        "\n",
        "print(\"Verilog Module:\")\n",
        "print(\"=\" * 80)\n",
        "print(binary_to_bcd_verilog_code)\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save the Verilog code\n",
        "with open(f\"{output_dir}/binary_to_bcd_converter.v\", \"w\") as f:\n",
        "    f.write(binary_to_bcd_verilog_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-CC_xn8dURm"
      },
      "source": [
        "### Run the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7atoKupMdPZL",
        "outputId": "373eae7e-0c83-416a-e32e-1736c5f51448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Running LLM-Aided Testbench Generation Pipeline for binary_to_bcd_converter\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "LLM-Aided Testbench Generation Pipeline\n",
            "================================================================================\n",
            "\n",
            "[Step 1-2] Input: Natural language description and Verilog code received\n",
            "Description length: 345 characters\n",
            "Verilog code length: 860 characters\n",
            "\n",
            "[Step 3] Generating testbench with test patterns...\n",
            "  - Generated testbench with 10 test patterns\n",
            "  - Module: binary_to_bcd_converter\n",
            "  - Saved initial testbench to: LLM-aided-Testbench-Generation/examples/output/binary_to_bcd/testbench_initial.v\n",
            "\n",
            "[Step 4] Generating Python golden model and computing expected outputs...\n",
            "  - Generated Python golden model (563 characters)\n",
            "  - Saved Python golden model to: LLM-aided-Testbench-Generation/examples/output/binary_to_bcd/golden_model.py\n",
            "  - Computing golden outputs for all test patterns...\n",
            "  - Successfully computed outputs for 10/10 patterns\n",
            "  - Saved test patterns with golden outputs to: LLM-aided-Testbench-Generation/examples/output/binary_to_bcd/test_patterns_with_golden.json\n",
            "\n",
            "[Step 5] Updating testbench with golden outputs and verification logic...\n",
            "  - Saved final testbench to: LLM-aided-Testbench-Generation/examples/output/binary_to_bcd/testbench_final.v\n",
            "\n",
            "================================================================================\n",
            "Pipeline completed successfully!\n",
            "================================================================================\n",
            "\n",
            "Generated files in 'LLM-aided-Testbench-Generation/examples/output/binary_to_bcd':\n",
            "  - testbench_initial.v    : Initial testbench with test patterns\n",
            "  - golden_model.py        : Python reference implementation\n",
            "  - test_patterns_with_golden.json : Test patterns with expected outputs\n",
            "  - testbench_final.v      : Final testbench with verification\n",
            "\n",
            "\n",
            "‚úì binary_to_bcd testbench generation completed successfully!\n",
            "\n",
            "Generated files are in: LLM-aided-Testbench-Generation/examples/output/binary_to_bcd/\n"
          ]
        }
      ],
      "source": [
        "# Run pipeline for the adder\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Running LLM-Aided Testbench Generation Pipeline for binary_to_bcd_converter\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    result = pipeline.run(\n",
        "        description=binary_to_bcd_description,\n",
        "        verilog_code=binary_to_bcd_verilog_code,\n",
        "        output_dir=f\"{output_dir}/binary_to_bcd\"\n",
        "    )\n",
        "    print(\"\\n‚úì binary_to_bcd testbench generation completed successfully!\")\n",
        "    print(f\"\\nGenerated files are in: {output_dir}/binary_to_bcd/\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚úó Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM5n4aVbeYcD"
      },
      "source": [
        "### Simulate the binary_to_bcd testbench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DqGQKf5eSji",
        "outputId": "f5c77700-bc24-4a55-924c-a374c958b8f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compiling binary_to_bcd testbench...\n",
            "‚úì Compilation successful\n",
            "\n",
            "Running simulation...\n",
            "Simulation Output:\n",
            "================================================================================\n",
            "Test 1: binary_input = 00000\n",
            "‚úì Test 1: bcd_output = 00000000 (expected 00000000)\n",
            "Test 2: binary_input = 11111\n",
            "‚úì Test 2: bcd_output = 00110001 (expected 00110001)\n",
            "Test 3: binary_input = 00001\n",
            "‚úì Test 3: bcd_output = 00000001 (expected 00000001)\n",
            "Test 4: binary_input = 11110\n",
            "‚úì Test 4: bcd_output = 00110000 (expected 00110000)\n",
            "Test 5: binary_input = 01010\n",
            "‚úì Test 5: bcd_output = 00010000 (expected 00010000)\n",
            "Test 6: binary_input = 01111\n",
            "‚úì Test 6: bcd_output = 00010101 (expected 00010101)\n",
            "Test 7: binary_input = 01010\n",
            "‚úì Test 7: bcd_output = 00010000 (expected 00010000)\n",
            "Test 8: binary_input = 10000\n",
            "‚úì Test 8: bcd_output = 00010110 (expected 00010110)\n",
            "Test 9: binary_input = 00011\n",
            "‚úì Test 9: bcd_output = 00000011 (expected 00000011)\n",
            "Test 10: binary_input = 10100\n",
            "‚úì Test 10: bcd_output = 00100000 (expected 00100000)\n",
            "Test Summary: Total Tests =          10, Passed =          10, Failed =           0\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Simulate the adder testbench\n",
        "try:\n",
        "    # Compile\n",
        "    print(\"Compiling binary_to_bcd testbench...\")\n",
        "    compile_result = subprocess.run(\n",
        "        f\"iverilog -g2012 -o {output_dir}/binary_to_bcd/sim.vvp {output_dir}/binary_to_bcd_converter.v {output_dir}/binary_to_bcd/testbench_final.v\",\n",
        "        shell=True,\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=30\n",
        "    )\n",
        "\n",
        "    if compile_result.returncode != 0:\n",
        "        print(f\"Compilation failed:\")\n",
        "        print(compile_result.stderr)\n",
        "    else:\n",
        "        print(\"‚úì Compilation successful\\n\")\n",
        "\n",
        "        # Run simulation\n",
        "        print(\"Running simulation...\")\n",
        "        sim_result = subprocess.run(\n",
        "            f\"vvp {output_dir}/binary_to_bcd/sim.vvp\",\n",
        "            shell=True,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=30\n",
        "        )\n",
        "\n",
        "        print(\"Simulation Output:\")\n",
        "        print(\"=\" * 80)\n",
        "        print(sim_result.stdout)\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"iverilog not found. Skipping simulation.\")\n",
        "except Exception as e:\n",
        "    print(f\"Simulation error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE57JKw5VUb_"
      },
      "source": [
        "## Section 11: Summary and Next Steps\n",
        "\n",
        "### What We've Accomplished\n",
        "\n",
        "In this notebook, we've demonstrated a complete LLM-aided testbench generation system that:\n",
        "\n",
        "1. ‚úÖ Accepts natural language descriptions and Verilog code\n",
        "2. ‚úÖ Generates comprehensive test patterns using LLM\n",
        "3. ‚úÖ Creates Python golden models for reference\n",
        "4. ‚úÖ Computes expected outputs automatically\n",
        "5. ‚úÖ Produces Verilog testbenches with full verification logic\n",
        "6. ‚úÖ Runs simulations to validate the design\n",
        "\n",
        "### Generated Artifacts\n",
        "\n",
        "For each module, the pipeline produces:\n",
        "- **testbench_initial.v**: Testbench with test patterns (no verification)\n",
        "- **golden_model.py**: Python reference implementation\n",
        "- **test_patterns_with_golden.json**: Test data with expected outputs\n",
        "- **testbench_final.v**: Complete testbench with verification logic\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "You can extend this system by:\n",
        "- Adding support for sequential circuits and FSMs\n",
        "- Implementing coverage-driven test generation\n",
        "- Integrating with formal verification tools\n",
        "- Adding waveform generation and analysis\n",
        "- Supporting SystemVerilog and UVM testbenches\n",
        "\n",
        "### Using Your Own Modules\n",
        "\n",
        "To generate testbenches for your own Verilog modules:\n",
        "\n",
        "1. Prepare a clear natural language description\n",
        "2. Provide the Verilog module code\n",
        "3. Run the pipeline:\n",
        "   ```python\n",
        "   result = pipeline.run(\n",
        "       description=your_description,\n",
        "       verilog_code=your_verilog_code,\n",
        "       output_dir=\"your_output_dir\"\n",
        "   )\n",
        "   ```\n",
        "\n",
        "### Resources\n",
        "\n",
        "- **Repository**: [github.com/FCHXWH823/LLM-aided-Testbench-Generation](https://github.com/FCHXWH823/LLM-aided-Testbench-Generation)\n",
        "- **Documentation**: See README.md and USAGE_GUIDE.md in the repository\n",
        "- **Examples**: Check the `examples/` directory for more samples\n",
        "\n",
        "Thank you for using LLM-Aided Testbench Generation! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkILXmR5VUb_"
      },
      "source": [
        "## Appendix: View All Generated Files\n",
        "\n",
        "List all files generated during this notebook session:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Zsf7OvyVUb_"
      },
      "outputs": [],
      "source": [
        "# List all generated files\n",
        "import os\n",
        "\n",
        "print(\"All Generated Files:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def list_files(directory, prefix=\"\"):\n",
        "    \"\"\"Recursively list all files in a directory\"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"{prefix}(Directory not found)\")\n",
        "        return\n",
        "\n",
        "    for item in sorted(os.listdir(directory)):\n",
        "        path = os.path.join(directory, item)\n",
        "        if os.path.isfile(path):\n",
        "            size = os.path.getsize(path)\n",
        "            print(f\"{prefix}{item:35s} ({size:,} bytes)\")\n",
        "        elif os.path.isdir(path):\n",
        "            print(f\"{prefix}{item}/\")\n",
        "            list_files(path, prefix + \"  \")\n",
        "\n",
        "list_files(output_dir)\n",
        "print(\"=\" * 80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
