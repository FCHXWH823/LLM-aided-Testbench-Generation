{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Aided Testbench Generation\n\n## Overview\n\nThis notebook demonstrates a complete **LLM-aided testbench generation system** for Verilog hardware designs. The system automates the creation of comprehensive testbenches with golden reference outputs using a 5-step pipeline:\n\n1. **Step 1-2**: Accept natural language description and Verilog code\n2. **Step 3**: Generate testbench with comprehensive test patterns using LLM\n3. **Step 4**: Create Python golden model from description and compute expected outputs\n4. **Step 5**: Update testbench with verification logic and run simulation\n\n### Features\n\n- ðŸ¤– **LLM-Powered**: Uses GPT-4 to generate intelligent testbenches\n- ðŸŽ¯ **Comprehensive Testing**: Covers corner cases, boundary values, and random patterns\n- ðŸ” **Automatic Verification**: Built-in pass/fail checking and test summaries\n- ðŸ“ **Self-Contained**: All code included in this notebook for easy execution\n- âš¡ **Ready to Run**: Execute all cells to see the complete workflow\n\nThis notebook is completely self-contained - you can run it directly without any external dependencies (except the `openai` library)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Installation and Setup\n\nFirst, we install the required dependencies and set up the environment. The only external dependency is the OpenAI API client."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required packages\n!pip install openai>=0.27.0 -q\n\n# Import standard libraries\nimport os\nimport json\nimport sys\nimport subprocess\nfrom typing import Dict, Any, List, Optional\nimport re\n\nprint(\"âœ“ Dependencies installed successfully\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: API Key Configuration\n\nTo use the full LLM-powered generation, you need to set your OpenAI API key. You can either:\n1. Set it as an environment variable: `export OPENAI_API_KEY='your-api-key'`\n2. Directly set it in the cell below\n\n**Note**: Without an API key, the system will run in mock/demo mode for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set your OpenAI API key here (or use environment variable)\n# os.environ['OPENAI_API_KEY'] = 'your-api-key-here'\n\n# Check if API key is set\napi_key = os.environ.get('OPENAI_API_KEY', '')\nif api_key:\n    print(\"âœ“ OpenAI API key is configured\")\nelse:\n    print(\"âš  Warning: OpenAI API key not set. Running in demo mode.\")\n    print(\"  Set your API key with: os.environ['OPENAI_API_KEY'] = 'your-key'\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Create Output Directory\n\nWe'll create a directory to store all generated files (testbenches, golden models, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create output directory\noutput_dir = \"notebook_output\"\nos.makedirs(output_dir, exist_ok=True)\nprint(f\"âœ“ Output directory created: {output_dir}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: LLM Client Implementation\n\nThe `LLMClient` class handles all interactions with the OpenAI API. It provides a unified interface for generating text using GPT models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"\nLLM Client for interacting with language models.\nSupports multiple LLM providers (OpenAI, Anthropic, etc.)\n\"\"\"\n\nimport os\nimport json\nfrom typing import Optional, Dict, Any\nfrom xml.parsers.expat import model\nimport openai\n\n\nclass LLMClient:\n    \"\"\"Client for interacting with LLM APIs.\"\"\"\n    \n    def __init__(self, api_key: Optional[str] = None, model: str = \"gpt-4\", provider: str = \"openai\"):\n        \"\"\"\n        Initialize LLM client.\n        \n        Args:\n            api_key: API key for the LLM provider (if None, reads from environment)\n            model: Model name to use\n            provider: LLM provider ('openai', 'anthropic', etc.)\n        \"\"\"\n        self.provider = provider\n        self.model = model\n        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n        self.client = openai.OpenAI(api_key=api_key)\n    \n    def generate(self, prompt: str, system_prompt: Optional[str] = None, \n                 temperature: float = 0.7, max_tokens: int = 4000) -> str:\n        \"\"\"\n        Generate text using the LLM.\n        \n        Args:\n            prompt: User prompt\n            system_prompt: System prompt for the model\n            temperature: Sampling temperature (0-1)\n            max_tokens: Maximum tokens to generate\n            \n        Returns:\n            Generated text response\n        \"\"\"\n        try:\n            if self.provider == \"openai\":\n                messages = []\n                if system_prompt:\n                    messages.append({\"role\": \"system\", \"content\": system_prompt})\n                messages.append({\"role\": \"user\", \"content\": prompt})\n\n                response = self.client.chat.completions.create(\n                    model=self.model,\n                    messages=messages,\n                    temperature=temperature,\n                    max_tokens=max_tokens,\n                )\n                return response.choices[0].message.content\n            else:\n                raise ValueError(f\"Unsupported provider: {self.provider}\")\n        except Exception as e:\n            print(f\"Error generating response: {e}\")\n            return f\"Error: {str(e)}\"\n    \n    def is_available(self) -> bool:\n        \"\"\"Check if the LLM client is properly configured.\"\"\"\n        return self.api_key is not None and len(self.api_key) > 0\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Testbench Generator (Step 3)\n\nThe `TestbenchGenerator` class generates Verilog testbenches with comprehensive test patterns. It:\n- Extracts module information (name, inputs, outputs) from Verilog code\n- Uses LLM to generate comprehensive test patterns covering corner cases, boundary values, and random values\n- Creates a testbench skeleton without expected outputs (those come in Step 4)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"\nStep 3: Generate testbench with test patterns (without golden outputs).\n\"\"\"\n\nfrom typing import Dict, Any, List\nfrom .llm_client import LLMClient\n\n\nclass TestbenchGenerator:\n    \"\"\"Generate Verilog testbench with test patterns using LLM.\"\"\"\n    \n    def __init__(self, llm_client: LLMClient):\n        \"\"\"\n        Initialize testbench generator.\n        \n        Args:\n            llm_client: LLM client instance\n        \"\"\"\n        self.llm_client = llm_client\n    \n    def generate_testbench(self, description: str, verilog_code: str) -> Dict[str, Any]:\n        \"\"\"\n        Generate testbench with comprehensive test patterns.\n        \n        Args:\n            description: Natural language description of the Verilog module\n            verilog_code: Verilog code to be tested\n            \n        Returns:\n            Dictionary containing:\n                - testbench_code: Verilog testbench code (without expected outputs)\n                - test_patterns: List of test input patterns\n                - module_info: Information about module (name, inputs, outputs)\n        \"\"\"\n        # First, extract module information\n        module_info = self._extract_module_info(verilog_code)\n        \n        # Generate comprehensive test patterns\n        system_prompt = \"\"\"You are an expert in Verilog testbench generation. \nYour task is to generate comprehensive test patterns for a given Verilog module.\nGenerate test patterns that cover:\n1. All corner cases\n2. Boundary values\n3. Typical use cases\n4. Edge cases\n5. Random values for thorough testing\"\"\"\n\n        user_prompt = f\"\"\"Given the following Verilog module and its natural language description, \ngenerate a comprehensive Verilog testbench that includes ALL possible test patterns.\n\nNatural Language Description:\n{description}\n\nVerilog Module Code:\n{verilog_code}\n\nGenerate a Verilog testbench that:\n1. Declares all necessary signals\n2. Instantiates the module under test\n3. Includes a systematic set of test patterns covering all cases\n4. Uses $display to show inputs for each test (all $display statements are in the initial block and before $finish statement.)\n5. Does NOT include expected outputs or assertions yet (we will add those later)\n6. Numbers each test case\n\nPlease provide:\n1. The complete testbench code\n2. A list of test patterns in JSON format with test number and input values\n\nFormat your response as:\nTESTBENCH_CODE:\n```verilog\n[testbench code here]\n```\n\nTEST_PATTERNS (a list of dictionaries. Each dictionary contains only input signal names mapped to their values as plain binary strings (no prefixes like 0b, no spaces). Do not include any test_number field):\n```json\n[array of test patterns]\n```\n\"\"\"\n\n        response = self.llm_client.generate(user_prompt, system_prompt, max_tokens=4000)\n        \n        # Parse the response\n        testbench_code = self._extract_section(response, \"TESTBENCH_CODE:\", \"```verilog\", \"```\")\n        test_patterns_json = self._extract_section(response, \"TEST_PATTERNS:\", \"```json\", \"```\")\n        \n        try:\n            test_patterns = eval(test_patterns_json) if test_patterns_json else []\n        except:\n            test_patterns = []\n            print(\"Warning: Could not parse test patterns JSON\")\n        \n        return {\n            \"testbench_code\": testbench_code,\n            \"test_patterns\": test_patterns,\n            \"module_info\": module_info,\n            \"raw_response\": response\n        }\n    \n    def _extract_module_info(self, verilog_code: str) -> Dict[str, Any]:\n        \"\"\"\n        Extract module information (name, inputs, outputs) from Verilog code.\n        \n        Args:\n            verilog_code: Verilog module code\n            \n        Returns:\n            Dictionary with module information\n        \"\"\"\n        lines = verilog_code.strip().split('\\n')\n        module_name = \"\"\n        inputs = []\n        outputs = []\n        \n        for line in lines:\n            line = line.strip()\n            if line.startswith('module'):\n                # Extract module name\n                parts = line.split()\n                if len(parts) >= 2:\n                    module_name = parts[1].split('(')[0]\n            elif 'input' in line:\n                # Extract input signals\n                input_part = line.replace('input', '').replace(';', '').replace(',', '').strip()\n                if input_part:\n                    inputs.append(input_part)\n            elif 'output' in line:\n                # Extract output signals\n                output_part = line.replace('output', '').replace(';', '').replace(',', '').strip()\n                if output_part:\n                    outputs.append(output_part)\n        \n        return {\n            \"module_name\": module_name,\n            \"inputs\": inputs,\n            \"outputs\": outputs\n        }\n    \n    def _extract_section(self, text: str, marker: str, start_delim: str, end_delim: str) -> str:\n        \"\"\"\n        Extract a section from the LLM response between delimiters.\n        \n        Args:\n            text: Full response text\n            marker: Section marker to find\n            start_delim: Start delimiter (e.g., \"```verilog\")\n            end_delim: End delimiter (e.g., \"```\")\n            \n        Returns:\n            Extracted section content\n        \"\"\"\n        try:\n            # Find the marker\n            marker_idx = text.find(marker)\n            if marker_idx == -1:\n                return \"\"\n            \n            # Find the start delimiter after the marker\n            start_idx = text.find(start_delim, marker_idx)\n            if start_idx == -1:\n                return \"\"\n            start_idx += len(start_delim)\n            \n            # Find the end delimiter\n            end_idx = text.find(end_delim, start_idx)\n            if end_idx == -1:\n                return \"\"\n            \n            return text[start_idx:end_idx].strip()\n        except Exception as e:\n            print(f\"Error extracting section: {e}\")\n            return \"\"\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Golden Model Generator (Step 4)\n\nThe `GoldenModelGenerator` class creates a Python reference implementation from the natural language description. It:\n- Converts the description into executable Python code\n- Runs all test patterns through the Python model\n- Captures expected outputs for verification"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"\nStep 4: Generate Python golden model and compute golden outputs.\n\"\"\"\n\nimport sys\nimport io\nimport json\nfrom typing import Dict, Any, List\nfrom .llm_client import LLMClient\n\n\nclass GoldenModelGenerator:\n    \"\"\"Generate Python golden model and compute expected outputs.\"\"\"\n    \n    def __init__(self, llm_client: LLMClient):\n        \"\"\"\n        Initialize golden model generator.\n        \n        Args:\n            llm_client: LLM client instance\n        \"\"\"\n        self.llm_client = llm_client\n    \n    def generate_python_model(self, description: str, module_info: Dict[str, Any]) -> str:\n        \"\"\"\n        Generate Python implementation based on natural language description.\n        \n        Args:\n            description: Natural language description of the module\n            module_info: Module information (name, inputs, outputs)\n            \n        Returns:\n            Python code implementing the module functionality\n        \"\"\"\n        system_prompt = \"\"\"You are an expert in hardware design and Python programming.\nYour task is to create a Python function that implements the exact same functionality\nas described in the natural language specification.\"\"\"\n\n        user_prompt = f\"\"\"Given the following natural language description of a hardware module,\ncreate a Python function that implements this functionality.\n\nNatural Language Description:\n{description}\n\nModule Information:\n- Module Name: {module_info.get('module_name', 'unknown')}\n- Inputs: {module_info.get('inputs', [])}\n- Outputs: {module_info.get('outputs', [])}\n\nCreate a Python function named '{module_info.get('module_name', 'module')}_golden' that:\n1. Takes the input signals as parameters\n2. Computes and returns the output signals\n3. Implements the exact functionality described\n4. Handles all edge cases properly\n5. Returns outputs as a dictionary with output signal names as keys\n\nProvide ONLY the Python function code, no explanations.\nStart with 'def {module_info.get('module_name', 'module')}_golden(' and include complete implementation.\n\"\"\"\n\n        response = self.llm_client.generate(user_prompt, system_prompt, temperature=0.2, max_tokens=3000)\n        \n        # Extract Python code\n        python_code = self._extract_python_code(response)\n        \n        return python_code\n    \n    def compute_golden_outputs(self, python_code: str, test_patterns: List[Dict[str, Any]], \n                               module_info: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"\n        Execute the Python golden model with test patterns to get expected outputs.\n        \n        Args:\n            python_code: Python golden model code\n            test_patterns: List of test input patterns\n            module_info: Module information\n            \n        Returns:\n            List of test patterns with golden outputs added\n        \"\"\"\n        results = []\n        \n        # Execute the Python code in a safe namespace\n        namespace = {}\n        try:\n            exec(python_code, namespace)\n        except Exception as e:\n            print(f\"Error executing Python code: {e}\")\n            return results\n        \n        # Find the golden function\n        function_name = f\"{module_info.get('module_name', 'module')}_golden\"\n        golden_func = namespace.get(function_name)\n        \n        if not golden_func:\n            print(f\"Error: Could not find function {function_name}\")\n            return results\n        \n        # Run each test pattern through the golden model\n        for pattern in test_patterns:\n            try:\n                # Extract input values from the pattern\n                inputs = pattern.get('inputs', pattern)\n                \n                # Call the golden function\n                if isinstance(inputs, dict):\n                    outputs = golden_func(**inputs)\n                else:\n                    # If inputs is not a dict, try to call with positional args\n                    outputs = golden_func(*inputs.values()) if hasattr(inputs, 'values') else golden_func(inputs)\n                \n                # Add outputs to the pattern\n                result = pattern.copy()\n                result['expected_outputs'] = outputs\n                results.append(result)\n            except Exception as e:\n                print(f\"Error computing golden output for pattern {pattern}: {e}\")\n                result = pattern.copy()\n                result['expected_outputs'] = None\n                result['error'] = str(e)\n                results.append(result)\n        \n        return results\n    \n    def _extract_python_code(self, text: str) -> str:\n        \"\"\"\n        Extract Python code from LLM response.\n        \n        Args:\n            text: LLM response text\n            \n        Returns:\n            Extracted Python code\n        \"\"\"\n        # Try to find code between ```python and ```\n        if \"```python\" in text:\n            start = text.find(\"```python\") + len(\"```python\")\n            end = text.find(\"```\", start)\n            if end != -1:\n                return text[start:end].strip()\n        \n        # Try to find code between ``` and ```\n        if \"```\" in text:\n            parts = text.split(\"```\")\n            if len(parts) >= 3:\n                return parts[1].strip()\n        \n        # If no code blocks found, look for def statement\n        if \"def \" in text:\n            lines = text.split('\\n')\n            code_lines = []\n            in_function = False\n            for line in lines:\n                if line.strip().startswith('def '):\n                    in_function = True\n                if in_function:\n                    code_lines.append(line)\n            return '\\n'.join(code_lines)\n        \n        return text.strip()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Testbench Updater (Step 5)\n\nThe `TestbenchUpdater` class enhances the testbench with verification logic. It:\n- Injects expected outputs from the golden model\n- Adds pass/fail checking for each test case\n- Implements test summary reporting\n- Maintains proper Verilog formatting"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"\nStep 5: Update testbench with golden outputs.\n\"\"\"\n\nfrom typing import Dict, Any, List\nimport re\nimport json\nfrom .llm_client import LLMClient\n\n\nclass TestbenchUpdater:\n    \"\"\"Update generated testbench with golden outputs using LLM.\"\"\"\n    \n    def __init__(self, llm_client: LLMClient):\n        \"\"\"\n        Initialize testbench updater.\n        \n        Args:\n            llm_client: LLM client instance\n        \"\"\"\n        self.llm_client = llm_client\n    \n    def update_testbench(self, testbench_code: str, test_patterns_with_outputs: List[Dict[str, Any]], \n                        module_info: Dict[str, Any]) -> str:\n        \"\"\"\n        Update testbench code to include expected outputs and verification.\n        \n        Args:\n            testbench_code: Original testbench code without expected outputs\n            test_patterns_with_outputs: Test patterns with golden outputs\n            module_info: Module information\n            \n        Returns:\n            Updated testbench code with assertions and expected outputs\n        \"\"\"\n        # Use LLM if available, otherwise fall back to rule-based approach\n        if self.llm_client.is_available():\n            updated_code = self._llm_update_testbench(testbench_code, test_patterns_with_outputs, module_info)\n        else:\n            # Fallback to rule-based approach\n            updated_code = self._add_verification_logic(testbench_code, test_patterns_with_outputs, module_info)\n        \n        return updated_code\n    \n    def _llm_update_testbench(self, testbench_code: str, test_patterns_with_outputs: List[Dict[str, Any]], \n                             module_info: Dict[str, Any]) -> str:\n        \"\"\"\n        Use LLM to update testbench with verification logic.\n        \n        Args:\n            testbench_code: Original testbench code\n            test_patterns_with_outputs: Test patterns with golden outputs\n            module_info: Module information\n            \n        Returns:\n            Updated testbench code with verification logic\n        \"\"\"\n        system_prompt = \"\"\"You are an expert in Verilog testbench development and verification.\nYour task is to update a testbench by adding comprehensive verification logic and expected output checks.\nYou should:\n1. Add verification for each test case using the provided expected outputs\n2. Track passed and failed test counts\n3. Display clear pass/fail messages for each output signal\n4. Generate a comprehensive test summary at the end\n5. Maintain the original testbench structure and style\n6. Use proper Verilog syntax and best practices\"\"\"\n\n        # Prepare test patterns data for the LLM\n        patterns_str = json.dumps(test_patterns_with_outputs, indent=2)\n        \n        user_prompt = f\"\"\"Given the following Verilog testbench and test patterns with expected outputs,\nupdate the testbench to include verification logic that checks the actual outputs against the expected outputs.\n\nOriginal Testbench Code:\n```verilog\n{testbench_code}\n```\n\nModule Information:\n- Module Name: {module_info.get('module_name', 'unknown')}\n- Inputs: {module_info.get('inputs', [])}\n- Outputs: {module_info.get('outputs', [])}\n\nTest Patterns with Expected Outputs:\n```json\n{patterns_str}\n```\n\nPlease update the testbench to:\n1. Add integer variables 'passed_tests' and 'failed_tests' at the beginning of the initial block (initialized to 0)\n2. After each test case (identified by $display statements), add a delay (#10) for outputs to settle\n3. For each output signal, compare the actual value against the expected value from the test patterns\n4. Display \"âœ“\" for passing checks and \"âœ—\" for failing checks with actual and expected values\n5. Increment passed_tests for each passing check and failed_tests for each failing check\n6. At the end of the initial block (before 'end'), add a test summary showing:\n   - Total tests run\n   - Number passed\n   - Number failed\n7. Preserve all original test case displays and structure\n8. Use proper indentation and formatting\n\nProvide ONLY the complete updated testbench code, no explanations.\nFormat your response as:\n```verilog\n[updated testbench code here]\n```\n\"\"\"\n\n        response = self.llm_client.generate(user_prompt, system_prompt, max_tokens=8000)\n        \n        # Extract the Verilog code from the response\n        updated_code = self._extract_verilog_code(response)\n        \n        # If extraction failed, fall back to original with rule-based update\n        if not updated_code or len(updated_code) < len(testbench_code) // 2:\n            print(\"Warning: LLM response extraction failed, using rule-based approach\")\n            updated_code = self._add_verification_logic(testbench_code, test_patterns_with_outputs, module_info)\n        \n        return updated_code\n    \n    def _extract_verilog_code(self, text: str) -> str:\n        \"\"\"\n        Extract Verilog code from LLM response.\n        \n        Args:\n            text: LLM response text\n            \n        Returns:\n            Extracted Verilog code\n        \"\"\"\n        # Try to find code between ```verilog and ```\n        if \"```verilog\" in text:\n            start = text.find(\"```verilog\") + len(\"```verilog\")\n            end = text.find(\"```\", start)\n            if end != -1:\n                return text[start:end].strip()\n        \n        # Try to find code between ``` and ```\n        if \"```\" in text:\n            parts = text.split(\"```\")\n            if len(parts) >= 3:\n                # Get the first code block\n                code = parts[1].strip()\n                # If it starts with a language identifier, remove it\n                if code.startswith(\"verilog\\n\") or code.startswith(\"verilog \"):\n                    code = code.split('\\n', 1)[1] if '\\n' in code else code\n                return code.strip()\n        \n        # If no code blocks found, look for module or testbench keywords\n        if \"module \" in text or \"initial \" in text:\n            return text.strip()\n        \n        return \"\"\n    \n    def _add_verification_logic(self, testbench_code: str, test_patterns: List[Dict[str, Any]], \n                               module_info: Dict[str, Any]) -> str:\n        \"\"\"\n        Add verification logic to the testbench.\n        \n        Args:\n            testbench_code: Original testbench code\n            test_patterns: Test patterns with expected outputs\n            module_info: Module information\n            \n        Returns:\n            Testbench code with verification logic added\n        \"\"\"\n        lines = testbench_code.split('\\n')\n        updated_lines = []\n        \n        # Track if we're in the initial block\n        in_initial = False\n        indent_level = 0\n        test_case_num = 0\n        \n        for i, line in enumerate(lines):\n            stripped = line.strip()\n            \n            # Detect initial block\n            if 'initial' in stripped and 'begin' in stripped:\n                in_initial = True\n                updated_lines.append(line)\n                # Add test result tracking variables after initial begin\n                updated_lines.append(\"    integer passed_tests = 0;\")\n                updated_lines.append(\"    integer failed_tests = 0;\")\n                updated_lines.append(\"\")\n                continue\n            \n            # Check for test case markers (e.g., $display for test cases)\n            if in_initial and '$display' in stripped and ('Test' in stripped or 'test' in stripped):\n                # This is likely a test case display\n                updated_lines.append(line)\n                \n                # Add delay to let outputs settle\n                indent = len(line) - len(line.lstrip())\n                indent_str = ' ' * indent\n                updated_lines.append(f\"{indent_str}#10; // Wait for outputs to settle\")\n                \n                # Add verification for this test case if we have expected outputs\n                if test_case_num < len(test_patterns):\n                    pattern = test_patterns[test_case_num]\n                    if 'expected_outputs' in pattern and pattern['expected_outputs']:\n                        verification_lines = self._generate_verification(\n                            pattern, module_info, indent\n                        )\n                        updated_lines.extend(verification_lines)\n                    test_case_num += 1\n                continue\n            \n            # Check for end of initial block\n            if in_initial and (stripped == 'end' or stripped.startswith('end')):\n                # Add final summary before the end\n                indent = len(line) - len(line.lstrip())\n                indent_str = ' ' * indent\n                updated_lines.append(\"\")\n                updated_lines.append(f\"{indent_str}// Test Summary\")\n                updated_lines.append(f'{indent_str}$display(\"\\\\n========== Test Summary ==========\");')\n                updated_lines.append(f'{indent_str}$display(\"Total Tests: %0d\", passed_tests + failed_tests);')\n                updated_lines.append(f'{indent_str}$display(\"Passed: %0d\", passed_tests);')\n                updated_lines.append(f'{indent_str}$display(\"Failed: %0d\", failed_tests);')\n                updated_lines.append(f'{indent_str}$display(\"==================================\\\\n\");')\n                updated_lines.append(\"\")\n                updated_lines.append(line)\n                in_initial = False\n                continue\n            \n            updated_lines.append(line)\n        \n        return '\\n'.join(updated_lines)\n    \n    def _generate_verification(self, pattern: Dict[str, Any], module_info: Dict[str, Any], \n                              indent: int) -> List[str]:\n        \"\"\"\n        Generate verification code for a single test pattern.\n        \n        Args:\n            pattern: Test pattern with expected outputs\n            expected_outputs: Expected output values\n            module_info: Module information\n            indent: Indentation level\n            \n        Returns:\n            List of verification code lines\n        \"\"\"\n        lines = []\n        indent_str = ' ' * indent\n        \n        expected = pattern.get('expected_outputs', {})\n        if not expected:\n            return lines\n        \n        # Get output signals\n        outputs = module_info.get('outputs', [])\n        \n        # Generate verification for each output\n        for output in outputs:\n            output_name = output.split('[')[0].strip()  # Remove bit width if present\n            output_name = output_name.split()[-1]  # Get the signal name\n            \n            if output_name in expected:\n                expected_value = expected[output_name]\n                \n                # Check if expected value is boolean\n                if isinstance(expected_value, bool):\n                    expected_value = 1 if expected_value else 0\n                \n                # Generate comparison\n                lines.append(f\"{indent_str}if ({output_name} === {expected_value}) begin\")\n                lines.append(f'{indent_str}    $display(\"  âœ“ {output_name} = %b (expected: {expected_value})\", {output_name});')\n                lines.append(f\"{indent_str}    passed_tests = passed_tests + 1;\")\n                lines.append(f\"{indent_str}end else begin\")\n                lines.append(f'{indent_str}    $display(\"  âœ— {output_name} = %b (expected: {expected_value})\", {output_name});')\n                lines.append(f\"{indent_str}    failed_tests = failed_tests + 1;\")\n                lines.append(f\"{indent_str}end\")\n        \n        return lines"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Pipeline Orchestrator\n\nThe `TestbenchPipeline` class coordinates all steps in the generation process. It:\n- Manages the flow from description to final testbench\n- Handles file I/O for all artifacts\n- Provides progress reporting\n- Implements graceful degradation when LLM is unavailable"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"\nMain pipeline orchestrating the entire testbench generation process.\n\"\"\"\n\nimport json\nimport os\nfrom typing import Dict, Any, Optional\nfrom .llm_client import LLMClient\nfrom .testbench_generator import TestbenchGenerator\nfrom .golden_model_generator import GoldenModelGenerator\nfrom .testbench_updater import TestbenchUpdater\nimport subprocess\n\nclass TestbenchPipeline:\n    \"\"\"\n    Main pipeline for LLM-aided testbench generation.\n    \n    Steps:\n    1. Accept natural language description and Verilog code\n    2. Generate testbench with test patterns (no golden outputs)\n    3. Generate Python golden model from description\n    4. Execute golden model with test patterns to get expected outputs\n    5. Update testbench with golden outputs and verification logic\n    \"\"\"\n    \n    def __init__(self, api_key: Optional[str] = None, model: str = \"gpt-4\", provider: str = \"openai\"):\n        \"\"\"\n        Initialize the pipeline.\n        \n        Args:\n            api_key: API key for LLM provider\n            model: Model name to use\n            provider: LLM provider name\n        \"\"\"\n        self.llm_client = LLMClient(api_key, model, provider)\n        self.testbench_gen = TestbenchGenerator(self.llm_client)\n        self.golden_gen = GoldenModelGenerator(self.llm_client)\n        self.testbench_updater = TestbenchUpdater(self.llm_client)\n        \n    def run(self, description: str, verilog_code: str, output_dir: str = \"output\") -> Dict[str, Any]:\n        \"\"\"\n        Run the complete testbench generation pipeline.\n        \n        Args:\n            description: Natural language description of the module\n            verilog_code: Verilog code to be tested\n            output_dir: Directory to save output files\n            \n        Returns:\n            Dictionary containing all generated artifacts\n        \"\"\"\n        print(\"=\" * 80)\n        print(\"LLM-Aided Testbench Generation Pipeline\")\n        print(\"=\" * 80)\n        \n        # Create output directory\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Step 1 & 2: Input handling (description and verilog code are already provided)\n        print(\"\\n[Step 1-2] Input: Natural language description and Verilog code received\")\n        print(f\"Description length: {len(description)} characters\")\n        print(f\"Verilog code length: {len(verilog_code)} characters\")\n        \n        # Step 3: Generate testbench with test patterns\n        print(\"\\n[Step 3] Generating testbench with test patterns...\")\n        if not self.llm_client.is_available():\n            print(\"WARNING: LLM client not configured. Using mock generation.\")\n            testbench_result = self._mock_testbench_generation(verilog_code)\n        else:\n            testbench_result = self.testbench_gen.generate_testbench(description, verilog_code)\n        \n        print(f\"  - Generated testbench with {len(testbench_result['test_patterns'])} test patterns\")\n        print(f\"  - Module: {testbench_result['module_info']['module_name']}\")\n        \n        # Save initial testbench (without golden outputs)\n        initial_tb_path = os.path.join(output_dir, \"testbench_initial.v\")\n        with open(initial_tb_path, 'w') as f:\n            f.write(testbench_result['testbench_code'])\n        print(f\"  - Saved initial testbench to: {initial_tb_path}\")\n        \n        # Step 4: Generate Python golden model and compute golden outputs\n        print(\"\\n[Step 4] Generating Python golden model and computing expected outputs...\")\n        if not self.llm_client.is_available():\n            print(\"WARNING: LLM client not configured. Using mock golden model.\")\n            python_code = self._mock_python_model(testbench_result['module_info'])\n        else:\n            python_code = self.golden_gen.generate_python_model(\n                description, \n                testbench_result['module_info']\n            )\n        \n        print(f\"  - Generated Python golden model ({len(python_code)} characters)\")\n        \n        # change testbench_result['test_patterns'] value to integral values\n        patterns = []\n        for pattern in testbench_result['test_patterns']:\n            for key in pattern:\n                if isinstance(pattern[key], str):\n                    pattern[key] = int(pattern[key], 2)\n            patterns.append(pattern)\n        testbench_result['test_patterns'] = patterns\n\n        # Save Python golden model\n        python_path = os.path.join(output_dir, \"golden_model.py\")\n        with open(python_path, 'w') as f:\n            f.write(python_code)\n        print(f\"  - Saved Python golden model to: {python_path}\")\n        \n        # Compute golden outputs\n        print(\"  - Computing golden outputs for all test patterns...\")\n        test_patterns_with_outputs = self.golden_gen.compute_golden_outputs(\n            python_code,\n            testbench_result['test_patterns'],\n            testbench_result['module_info']\n        )\n        \n        successful_patterns = sum(1 for p in test_patterns_with_outputs \n                                 if 'expected_outputs' in p and p['expected_outputs'] is not None)\n        print(f\"  - Successfully computed outputs for {successful_patterns}/{len(test_patterns_with_outputs)} patterns\")\n        \n        # Save test patterns with golden outputs\n        patterns_path = os.path.join(output_dir, \"test_patterns_with_golden.json\")\n        with open(patterns_path, 'w') as f:\n            json.dump(test_patterns_with_outputs, f, indent=2)\n        print(f\"  - Saved test patterns with golden outputs to: {patterns_path}\")\n        \n        # Step 5: Update testbench with golden outputs\n        print(\"\\n[Step 5] Updating testbench with golden outputs and verification logic...\")\n        final_testbench = self.testbench_updater.update_testbench(\n            testbench_result['testbench_code'],\n            test_patterns_with_outputs,\n            testbench_result['module_info']\n        )\n        \n        # Save final testbench\n        final_tb_path = os.path.join(output_dir, \"testbench_final.v\")\n        with open(final_tb_path, 'w') as f:\n            f.write(final_testbench)\n        print(f\"  - Saved final testbench to: {final_tb_path}\")\n        \n        print(\"\\n\" + \"=\" * 80)\n        print(\"Pipeline completed successfully!\")\n        print(\"=\" * 80)\n        print(f\"\\nGenerated files in '{output_dir}':\")\n        print(f\"  - testbench_initial.v    : Initial testbench with test patterns\")\n        print(f\"  - golden_model.py        : Python reference implementation\")\n        print(f\"  - test_patterns_with_golden.json : Test patterns with expected outputs\")\n        print(f\"  - testbench_final.v      : Final testbench with verification\")\n        print()\n        \n        return {\n            'description': description,\n            'verilog_code': verilog_code,\n            'module_info': testbench_result['module_info'],\n            'test_patterns': testbench_result['test_patterns'],\n            'initial_testbench': testbench_result['testbench_code'],\n            'python_golden_model': python_code,\n            'test_patterns_with_outputs': test_patterns_with_outputs,\n            'final_testbench': final_testbench,\n            'output_dir': output_dir\n        }\n    \n    def _mock_testbench_generation(self, verilog_code: str) -> Dict[str, Any]:\n        \"\"\"Mock testbench generation when LLM is not available.\"\"\"\n        return {\n            'testbench_code': '// Mock testbench - LLM not configured\\n' + verilog_code,\n            'test_patterns': [],\n            'module_info': {\n                'module_name': 'unknown',\n                'inputs': [],\n                'outputs': []\n            }\n        }\n    \n    def _mock_python_model(self, module_info: Dict[str, Any]) -> str:\n        \"\"\"Mock Python model generation when LLM is not available.\"\"\"\n        return f\"# Mock Python model - LLM not configured\\ndef {module_info['module_name']}_golden():\\n    pass\\n\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Example 1 - 2-to-1 Multiplexer\n\nNow let's run a complete example! We'll generate a testbench for a simple 2-to-1 multiplexer.\n\n### Natural Language Description"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Natural language description of the module\nmux_description = \"\"\"\nA 2-to-1 multiplexer (MUX).\n\nThe module takes two 1-bit input signals (a and b) and one 1-bit select signal (sel).\n\nFunctionality:\n- Input 'a': First data input (1-bit)\n- Input 'b': Second data input (1-bit)\n- Input 'sel': Select signal (1-bit)\n- Output 'y': Selected output (1-bit)\n\nWhen sel is 0, the output y should be equal to input a.\nWhen sel is 1, the output y should be equal to input b.\n\nThis is a combinational logic circuit with no state or memory.\n\n\"\"\"\n\nprint(\"Natural Language Description:\")\nprint(\"=\" * 80)\nprint(mux_description)\nprint(\"=\" * 80)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verilog Module Code"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Verilog module to be tested\nmux_verilog_code = \"\"\"\nmodule mux2to1 (\n    input wire a,\n    input wire b,\n    input wire sel,\n    output wire y\n);\n    assign y = sel ? b : a;\nendmodule\n\n\"\"\"\n\nprint(\"Verilog Module:\")\nprint(\"=\" * 80)\nprint(mux_verilog_code)\nprint(\"=\" * 80)\n\n# Save the Verilog code for later simulation\nwith open(f\"{output_dir}/mux2to1.v\", \"w\") as f:\n    f.write(mux_verilog_code)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Complete Pipeline\n\nNow we execute the 5-step pipeline to generate the testbench:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize and run the pipeline\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Running LLM-Aided Testbench Generation Pipeline for MUX\")\nprint(\"=\" * 80 + \"\\n\")\n\n# Create pipeline instance\npipeline = TestbenchPipeline(\n    api_key=os.environ.get('OPENAI_API_KEY'),\n    model='gpt-4o',\n    provider='openai'\n)\n\n# Run the complete pipeline\ntry:\n    result = pipeline.run(\n        description=mux_description,\n        verilog_code=mux_verilog_code,\n        output_dir=f\"{output_dir}/mux\"\n    )\n    print(\"\\nâœ“ MUX testbench generation completed successfully!\")\n    print(f\"\\nGenerated files are in: {output_dir}/mux/\")\nexcept Exception as e:\n    print(f\"\\nâœ— Error: {e}\")\n    import traceback\n    traceback.print_exc()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Generated Files\n\nLet's examine what was generated:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# List generated files\nimport os\n\nprint(\"Generated Files:\")\nprint(\"=\" * 80)\nmux_output_dir = f\"{output_dir}/mux\"\nif os.path.exists(mux_output_dir):\n    for filename in sorted(os.listdir(mux_output_dir)):\n        filepath = os.path.join(mux_output_dir, filename)\n        if os.path.isfile(filepath):\n            size = os.path.getsize(filepath)\n            print(f\"  - {filename:35s} ({size:,} bytes)\")\nelse:\n    print(\"  No files generated yet\")\nprint(\"=\" * 80)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the Initial Testbench\n\nThe initial testbench includes test patterns but not the expected outputs:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Display initial testbench (first 50 lines)\ntry:\n    with open(f\"{output_dir}/mux/testbench_initial.v\", \"r\") as f:\n        lines = f.readlines()\n        print(\"Initial Testbench (first 50 lines):\")\n        print(\"=\" * 80)\n        for i, line in enumerate(lines[:50], 1):\n            print(f\"{i:3d}: {line}\", end=\"\")\n        if len(lines) > 50:\n            print(f\"\\n... ({len(lines) - 50} more lines)\")\n        print(\"=\" * 80)\nexcept FileNotFoundError:\n    print(\"Initial testbench file not found\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the Python Golden Model\n\nThe golden model implements the expected behavior in Python:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Display Python golden model\ntry:\n    with open(f\"{output_dir}/mux/golden_model.py\", \"r\") as f:\n        golden_code = f.read()\n        print(\"Python Golden Model:\")\n        print(\"=\" * 80)\n        print(golden_code)\n        print(\"=\" * 80)\nexcept FileNotFoundError:\n    print(\"Golden model file not found\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Test Patterns with Expected Outputs\n\nThe test patterns JSON file contains all test cases with their expected outputs:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Display test patterns with golden outputs\ntry:\n    with open(f\"{output_dir}/mux/test_patterns_with_golden.json\", \"r\") as f:\n        patterns = json.load(f)\n        print(f\"Test Patterns with Golden Outputs ({len(patterns)} patterns):\")\n        print(\"=\" * 80)\n        for i, pattern in enumerate(patterns[:5], 1):  # Show first 5\n            print(f\"\\nPattern {i}:\")\n            print(f\"  Inputs:  {pattern.get('inputs', pattern)}\")\n            print(f\"  Expected: {pattern.get('expected_outputs', 'N/A')}\")\n        if len(patterns) > 5:\n            print(f\"\\n... ({len(patterns) - 5} more patterns)\")\n        print(\"=\" * 80)\nexcept FileNotFoundError:\n    print(\"Test patterns file not found\")\nexcept json.JSONDecodeError:\n    print(\"Error decoding test patterns JSON\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the Final Testbench with Verification\n\nThe final testbench includes all verification logic:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Display final testbench (first 60 lines)\ntry:\n    with open(f\"{output_dir}/mux/testbench_final.v\", \"r\") as f:\n        lines = f.readlines()\n        print(f\"Final Testbench with Verification ({len(lines)} lines total):\")\n        print(\"=\" * 80)\n        for i, line in enumerate(lines[:60], 1):\n            print(f\"{i:3d}: {line}\", end=\"\")\n        if len(lines) > 60:\n            print(f\"\\n... ({len(lines) - 60} more lines)\")\n        print(\"=\" * 80)\nexcept FileNotFoundError:\n    print(\"Final testbench file not found\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate the Testbench\n\nIf you have Icarus Verilog installed, we can simulate the testbench to see the results:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Simulate the testbench with Icarus Verilog\ntry:\n    # Compile\n    print(\"Compiling testbench...\")\n    compile_result = subprocess.run(\n        f\"iverilog -g2012 -o {output_dir}/mux/sim {output_dir}/mux2to1.v {output_dir}/mux/testbench_final.v\",\n        shell=True,\n        capture_output=True,\n        text=True,\n        timeout=30\n    )\n    \n    if compile_result.returncode != 0:\n        print(f\"Compilation failed:\")\n        print(compile_result.stderr)\n    else:\n        print(\"âœ“ Compilation successful\\n\")\n        \n        # Run simulation\n        print(\"Running simulation...\")\n        sim_result = subprocess.run(\n            f\"vvp {output_dir}/mux/sim\",\n            shell=True,\n            capture_output=True,\n            text=True,\n            timeout=30\n        )\n        \n        print(\"Simulation Output:\")\n        print(\"=\" * 80)\n        print(sim_result.stdout)\n        print(\"=\" * 80)\n        \n        if sim_result.stderr:\n            print(\"Warnings/Errors:\")\n            print(sim_result.stderr)\n            \nexcept subprocess.TimeoutExpired:\n    print(\"Simulation timed out\")\nexcept FileNotFoundError:\n    print(\"iverilog not found. Please install Icarus Verilog to run simulations.\")\n    print(\"On Ubuntu/Debian: sudo apt-get install iverilog\")\n    print(\"On macOS: brew install icarus-verilog\")\nexcept Exception as e:\n    print(f\"Simulation error: {e}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Example 2 - 4-bit Adder\n\nLet's try another example with a more complex module - a 4-bit adder with carry output.\n\n### Natural Language Description"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Natural language description of the 4-bit adder\nadder_description = \"\"\"\nA simple 4-bit adder module.\n\nThe module takes two 4-bit input signals (a and b) and produces a 4-bit sum output and a 1-bit carry output.\n\nFunctionality:\n- Input 'a': 4-bit unsigned number\n- Input 'b': 4-bit unsigned number  \n- Output 'sum': 4-bit result of a + b (lower 4 bits)\n- Output 'carry': 1-bit carry-out flag (set to 1 if result exceeds 15)\n\nThe adder performs unsigned addition of the two 4-bit inputs.\nIf the result is greater than 15 (0xF), the carry output should be set to 1.\n\n\"\"\"\n\nprint(\"Natural Language Description:\")\nprint(\"=\" * 80)\nprint(adder_description)\nprint(\"=\" * 80)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verilog Module Code"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Verilog module to be tested\nadder_verilog_code = \"\"\"\nmodule adder4bit (\n    input wire [3:0] a,\n    input wire [3:0] b,\n    output wire [3:0] sum,\n    output wire carry\n);\n    wire [4:0] result;\n    assign result = a + b;\n    assign sum = result[3:0];\n    assign carry = result[4];\nendmodule\n\n\"\"\"\n\nprint(\"Verilog Module:\")\nprint(\"=\" * 80)\nprint(adder_verilog_code)\nprint(\"=\" * 80)\n\n# Save the Verilog code\nwith open(f\"{output_dir}/adder4bit.v\", \"w\") as f:\n    f.write(adder_verilog_code)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Pipeline for Adder\n\nExecute the complete pipeline for the 4-bit adder:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run pipeline for the adder\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Running LLM-Aided Testbench Generation Pipeline for 4-bit Adder\")\nprint(\"=\" * 80 + \"\\n\")\n\ntry:\n    result = pipeline.run(\n        description=adder_description,\n        verilog_code=adder_verilog_code,\n        output_dir=f\"{output_dir}/adder\"\n    )\n    print(\"\\nâœ“ Adder testbench generation completed successfully!\")\n    print(f\"\\nGenerated files are in: {output_dir}/adder/\")\nexcept Exception as e:\n    print(f\"\\nâœ— Error: {e}\")\n    import traceback\n    traceback.print_exc()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate the Adder Testbench\n\nRun the simulation for the adder:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Simulate the adder testbench\ntry:\n    # Compile\n    print(\"Compiling adder testbench...\")\n    compile_result = subprocess.run(\n        f\"iverilog -g2012 -o {output_dir}/adder/sim {output_dir}/adder4bit.v {output_dir}/adder/testbench_final.v\",\n        shell=True,\n        capture_output=True,\n        text=True,\n        timeout=30\n    )\n    \n    if compile_result.returncode != 0:\n        print(f\"Compilation failed:\")\n        print(compile_result.stderr)\n    else:\n        print(\"âœ“ Compilation successful\\n\")\n        \n        # Run simulation\n        print(\"Running simulation...\")\n        sim_result = subprocess.run(\n            f\"vvp {output_dir}/adder/sim\",\n            shell=True,\n            capture_output=True,\n            text=True,\n            timeout=30\n        )\n        \n        print(\"Simulation Output:\")\n        print(\"=\" * 80)\n        print(sim_result.stdout)\n        print(\"=\" * 80)\n            \nexcept FileNotFoundError:\n    print(\"iverilog not found. Skipping simulation.\")\nexcept Exception as e:\n    print(f\"Simulation error: {e}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Summary and Next Steps\n\n### What We've Accomplished\n\nIn this notebook, we've demonstrated a complete LLM-aided testbench generation system that:\n\n1. âœ… Accepts natural language descriptions and Verilog code\n2. âœ… Generates comprehensive test patterns using LLM\n3. âœ… Creates Python golden models for reference\n4. âœ… Computes expected outputs automatically\n5. âœ… Produces Verilog testbenches with full verification logic\n6. âœ… Runs simulations to validate the design\n\n### Generated Artifacts\n\nFor each module, the pipeline produces:\n- **testbench_initial.v**: Testbench with test patterns (no verification)\n- **golden_model.py**: Python reference implementation\n- **test_patterns_with_golden.json**: Test data with expected outputs\n- **testbench_final.v**: Complete testbench with verification logic\n\n### Next Steps\n\nYou can extend this system by:\n- Adding support for sequential circuits and FSMs\n- Implementing coverage-driven test generation\n- Integrating with formal verification tools\n- Adding waveform generation and analysis\n- Supporting SystemVerilog and UVM testbenches\n\n### Using Your Own Modules\n\nTo generate testbenches for your own Verilog modules:\n\n1. Prepare a clear natural language description\n2. Provide the Verilog module code\n3. Run the pipeline:\n   ```python\n   result = pipeline.run(\n       description=your_description,\n       verilog_code=your_verilog_code,\n       output_dir=\"your_output_dir\"\n   )\n   ```\n\n### Resources\n\n- **Repository**: [github.com/FCHXWH823/LLM-aided-Testbench-Generation](https://github.com/FCHXWH823/LLM-aided-Testbench-Generation)\n- **Documentation**: See README.md and USAGE_GUIDE.md in the repository\n- **Examples**: Check the `examples/` directory for more samples\n\nThank you for using LLM-Aided Testbench Generation! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: View All Generated Files\n\nList all files generated during this notebook session:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# List all generated files\nimport os\n\nprint(\"All Generated Files:\")\nprint(\"=\" * 80)\n\ndef list_files(directory, prefix=\"\"):\n    \"\"\"Recursively list all files in a directory\"\"\"\n    if not os.path.exists(directory):\n        print(f\"{prefix}(Directory not found)\")\n        return\n        \n    for item in sorted(os.listdir(directory)):\n        path = os.path.join(directory, item)\n        if os.path.isfile(path):\n            size = os.path.getsize(path)\n            print(f\"{prefix}{item:35s} ({size:,} bytes)\")\n        elif os.path.isdir(path):\n            print(f\"{prefix}{item}/\")\n            list_files(path, prefix + \"  \")\n\nlist_files(output_dir)\nprint(\"=\" * 80)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}