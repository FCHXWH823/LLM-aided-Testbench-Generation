{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-aided Testbench Generation - Complete Tutorial\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to the **LLM-aided Testbench Generation** tutorial! This notebook provides an interactive, hands-on demonstration of an automated testbench generation system that leverages Large Language Models (LLMs) to create comprehensive Verilog testbenches.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "In this notebook, you will:\n",
    "- Understand the 5-step automated pipeline for testbench generation\n",
    "- See how natural language descriptions are transformed into working testbenches\n",
    "- Generate testbenches for two example Verilog modules (a multiplexer and an adder)\n",
    "- Learn how golden reference models are created and used for verification\n",
    "- Explore the generated files and understand their purpose\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Basic understanding of digital logic design\n",
    "- Familiarity with Verilog (helpful but not required)\n",
    "- Python 3.7 or higher\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Testbench**: A Verilog module that instantiates and tests another module by applying input patterns and checking outputs.\n",
    "\n",
    "**Golden Model**: A reference implementation (in Python) that defines the expected behavior of the module under test.\n",
    "\n",
    "**Test Patterns**: A comprehensive set of input combinations designed to thoroughly verify the module's functionality.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Environment Setup\n",
    "\n",
    "### Introduction to Setup\n",
    "\n",
    "Before we can generate testbenches, we need to set up our environment. This section installs the necessary Python packages and imports the required modules from our testbench generation system.\n",
    "\n",
    "The main components we'll use are:\n",
    "- **TestbenchPipeline**: The orchestrator that runs the complete generation process\n",
    "- **LLMClient**: Interfaces with the Language Model (optional for this demo)\n",
    "\n",
    "**Note**: This demo can run in two modes:\n",
    "1. **Full LLM mode**: Requires an OpenAI API key for real LLM-powered generation\n",
    "2. **Demo mode**: Works without an API key using mock generation for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import openai\n",
    "    print(\"✓ OpenAI package is already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing required packages...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"openai>=0.27.0\"])\n",
    "    print(\"✓ Packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import json\n",
    "from src.testbench_pipeline import TestbenchPipeline\n",
    "from src.llm_client import LLMClient\n",
    "\n",
    "print(\"✓ All modules imported successfully\")\n",
    "print(\"\\nReady to generate testbenches!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Configure LLM API Key\n",
    "\n",
    "If you have an OpenAI API key and want to use full LLM-powered generation, uncomment and run the cell below. Otherwise, the system will use mock generation which is sufficient for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line and add your API key if you want to use real LLM generation\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-api-key-here'\n",
    "\n",
    "# Check if LLM is configured\n",
    "llm = LLMClient()\n",
    "if llm.is_available():\n",
    "    print(\"✓ LLM is configured and ready to use\")\n",
    "else:\n",
    "    print(\"ℹ Running in DEMO MODE (without LLM)\")\n",
    "    print(\"  The examples will still work with mock generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Understanding the Pipeline\n",
    "\n",
    "### Introduction to the 5-Step Pipeline\n",
    "\n",
    "The LLM-aided Testbench Generation system follows a systematic 5-step process:\n",
    "\n",
    "**Step 1-2: Input Handling**\n",
    "- Accept a natural language description of what the module should do\n",
    "- Accept the Verilog code to be tested (which may contain bugs)\n",
    "\n",
    "**Step 3: Testbench Generation**\n",
    "- Analyze the Verilog module structure (inputs, outputs, bit widths)\n",
    "- Use LLM to generate comprehensive test patterns covering corner cases, boundary values, typical cases, and random values\n",
    "- Create an initial testbench that applies these patterns but doesn't know the expected outputs yet\n",
    "\n",
    "**Step 4: Golden Model Creation**\n",
    "- Use LLM to convert the natural language description into executable Python code (the \"golden model\")\n",
    "- Run all test patterns through this Python model to compute the expected outputs\n",
    "- This gives us the \"golden\" reference outputs for verification\n",
    "\n",
    "**Step 5: Testbench Enhancement**\n",
    "- Inject the golden outputs into the testbench\n",
    "- Add verification logic that compares actual vs. expected outputs\n",
    "- Include pass/fail checking and test summary reporting\n",
    "\n",
    "### Why This Approach?\n",
    "\n",
    "This approach has several advantages:\n",
    "- **Automation**: Reduces manual effort in writing testbenches\n",
    "- **Comprehensive Coverage**: LLM helps identify important test cases\n",
    "- **Language-based Specification**: Uses natural language for clear specifications\n",
    "- **Separation of Concerns**: The golden model is independent of the Verilog implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Example 1 - 2-to-1 Multiplexer\n",
    "\n",
    "### Introduction to the Multiplexer Example\n",
    "\n",
    "A multiplexer (MUX) is one of the most fundamental building blocks in digital circuits. It selects one of several input signals and forwards it to a single output based on a control signal.\n",
    "\n",
    "Our example is a simple 2-to-1 MUX with:\n",
    "- Two data inputs: `a` and `b` (1-bit each)\n",
    "- One select input: `sel` (1-bit)\n",
    "- One output: `y` (1-bit)\n",
    "\n",
    "**Behavior**: When `sel` is 0, output `y` equals input `a`. When `sel` is 1, output `y` equals input `b`.\n",
    "\n",
    "This is an excellent starting example because:\n",
    "- It's simple and easy to understand\n",
    "- It has only 8 possible input combinations (2³)\n",
    "- The expected behavior is straightforward\n",
    "- It demonstrates pure combinational logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the natural language description for the 2-to-1 MUX\n",
    "mux_description = \"\"\"A 2-to-1 multiplexer (MUX).\n",
    "\n",
    "The module takes two 1-bit input signals (a and b) and one 1-bit select signal (sel).\n",
    "\n",
    "Functionality:\n",
    "- Input 'a': First data input (1-bit)\n",
    "- Input 'b': Second data input (1-bit)\n",
    "- Input 'sel': Select signal (1-bit)\n",
    "- Output 'y': Selected output (1-bit)\n",
    "\n",
    "When sel is 0, the output y should be equal to input a.\n",
    "When sel is 1, the output y should be equal to input b.\n",
    "\n",
    "This is a combinational logic circuit with no state or memory.\"\"\"\n",
    "\n",
    "print(\"Natural Language Description:\")\n",
    "print(\"=\" * 70)\n",
    "print(mux_description)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Verilog code for the 2-to-1 MUX\n",
    "mux_verilog = \"\"\"module mux2to1 (\n",
    "    input wire a,\n",
    "    input wire b,\n",
    "    input wire sel,\n",
    "    output wire y\n",
    ");\n",
    "    assign y = sel ? b : a;\n",
    "endmodule\"\"\"\n",
    "\n",
    "print(\"Verilog Code:\")\n",
    "print(\"=\" * 70)\n",
    "print(mux_verilog)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the testbench generation pipeline for the MUX\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Starting Testbench Generation for 2-to-1 Multiplexer\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = TestbenchPipeline()\n",
    "\n",
    "# Run the complete pipeline\n",
    "mux_result = pipeline.run(\n",
    "    description=mux_description,\n",
    "    verilog_code=mux_verilog,\n",
    "    output_dir=\"notebook_output/mux\"\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Testbench generation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the Generated Files\n",
    "\n",
    "Let's explore what was generated. The pipeline creates four main files:\n",
    "\n",
    "1. **testbench_initial.v**: The initial testbench with test patterns but no expected outputs\n",
    "2. **golden_model.py**: The Python reference implementation\n",
    "3. **test_patterns_with_golden.json**: Test patterns with expected outputs\n",
    "4. **testbench_final.v**: The complete testbench with verification logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated Python golden model\n",
    "print(\"Generated Python Golden Model:\")\n",
    "print(\"=\" * 70)\n",
    "with open(\"notebook_output/mux/golden_model.py\", \"r\") as f:\n",
    "    golden_model_code = f.read()\n",
    "    print(golden_model_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the test patterns with golden outputs\n",
    "print(\"\\nTest Patterns with Expected Outputs:\")\n",
    "print(\"=\" * 70)\n",
    "with open(\"notebook_output/mux/test_patterns_with_golden.json\", \"r\") as f:\n",
    "    test_patterns = json.load(f)\n",
    "    print(json.dumps(test_patterns, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(\"\\nTestbench Generation Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Module Name: {mux_result['module_info']['module_name']}\")\n",
    "print(f\"Number of Inputs: {len(mux_result['module_info']['inputs'])}\")\n",
    "print(f\"Number of Outputs: {len(mux_result['module_info']['outputs'])}\")\n",
    "print(f\"Total Test Patterns: {len(test_patterns)}\")\n",
    "successful_tests = sum(1 for p in test_patterns if p.get('expected_outputs'))\n",
    "print(f\"Successful Pattern Computations: {successful_tests}/{len(test_patterns)}\")\n",
    "print(f\"\\nOutput Directory: {mux_result['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Example 2 - 4-bit Adder\n",
    "\n",
    "### Introduction to the Adder Example\n",
    "\n",
    "Now let's try a more complex example: a 4-bit unsigned adder. This demonstrates how the system handles:\n",
    "- Multi-bit signals (4-bit inputs and outputs)\n",
    "- Arithmetic operations\n",
    "- Multiple outputs (sum and carry)\n",
    "- Overflow detection\n",
    "\n",
    "The 4-bit adder:\n",
    "- Takes two 4-bit unsigned inputs: `a` and `b` (range: 0-15 each)\n",
    "- Produces a 4-bit sum output: `sum` (lower 4 bits of the result)\n",
    "- Produces a 1-bit carry output: `carry` (1 if result > 15, else 0)\n",
    "\n",
    "**Example**: If `a=15` (0xF) and `b=1`, then `sum=0` (0x0) and `carry=1` (overflow occurred)\n",
    "\n",
    "This example is more interesting because:\n",
    "- It has 256 possible input combinations (2⁸)\n",
    "- It tests boundary conditions (overflow cases)\n",
    "- It requires understanding of binary arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the natural language description for the 4-bit adder\n",
    "adder_description = \"\"\"A simple 4-bit adder module.\n",
    "\n",
    "The module takes two 4-bit input signals (a and b) and produces a 4-bit sum output and a 1-bit carry output.\n",
    "\n",
    "Functionality:\n",
    "- Input 'a': 4-bit unsigned number\n",
    "- Input 'b': 4-bit unsigned number  \n",
    "- Output 'sum': 4-bit result of a + b (lower 4 bits)\n",
    "- Output 'carry': 1-bit carry-out flag (set to 1 if result exceeds 15)\n",
    "\n",
    "The adder performs unsigned addition of the two 4-bit inputs.\n",
    "If the result is greater than 15 (0xF), the carry output should be set to 1.\"\"\"\n",
    "\n",
    "print(\"Natural Language Description:\")\n",
    "print(\"=\" * 70)\n",
    "print(adder_description)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Verilog code for the 4-bit adder\n",
    "adder_verilog = \"\"\"module adder4bit (\n",
    "    input wire [3:0] a,\n",
    "    input wire [3:0] b,\n",
    "    output wire [3:0] sum,\n",
    "    output wire carry\n",
    ");\n",
    "    wire [4:0] result;\n",
    "    assign result = a + b;\n",
    "    assign sum = result[3:0];\n",
    "    assign carry = result[4];\n",
    "endmodule\"\"\"\n",
    "\n",
    "print(\"Verilog Code:\")\n",
    "print(\"=\" * 70)\n",
    "print(adder_verilog)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the testbench generation pipeline for the adder\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Starting Testbench Generation for 4-bit Adder\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Initialize pipeline (can reuse or create new instance)\n",
    "pipeline_adder = TestbenchPipeline()\n",
    "\n",
    "# Run the complete pipeline\n",
    "adder_result = pipeline_adder.run(\n",
    "    description=adder_description,\n",
    "    verilog_code=adder_verilog,\n",
    "    output_dir=\"notebook_output/adder\"\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Testbench generation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated Python golden model for the adder\n",
    "print(\"Generated Python Golden Model:\")\n",
    "print(\"=\" * 70)\n",
    "with open(\"notebook_output/adder/golden_model.py\", \"r\") as f:\n",
    "    adder_golden_model = f.read()\n",
    "    print(adder_golden_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample of test patterns with golden outputs\n",
    "print(\"\\nSample Test Patterns (first 10):\")\n",
    "print(\"=\" * 70)\n",
    "with open(\"notebook_output/adder/test_patterns_with_golden.json\", \"r\") as f:\n",
    "    adder_patterns = json.load(f)\n",
    "    # Show first 10 patterns\n",
    "    print(json.dumps(adder_patterns[:10], indent=2))\n",
    "    print(f\"\\n... and {len(adder_patterns) - 10} more patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics for the adder\n",
    "print(\"\\nTestbench Generation Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Module Name: {adder_result['module_info']['module_name']}\")\n",
    "print(f\"Number of Inputs: {len(adder_result['module_info']['inputs'])}\")\n",
    "print(f\"Number of Outputs: {len(adder_result['module_info']['outputs'])}\")\n",
    "print(f\"Total Test Patterns: {len(adder_patterns)}\")\n",
    "successful_adder_tests = sum(1 for p in adder_patterns if p.get('expected_outputs'))\n",
    "print(f\"Successful Pattern Computations: {successful_adder_tests}/{len(adder_patterns)}\")\n",
    "print(f\"\\nOutput Directory: {adder_result['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Understanding Generated Files\n",
    "\n",
    "### Introduction to Output Files\n",
    "\n",
    "The testbench generation pipeline produces four types of files. Let's examine each one to understand its purpose and structure.\n",
    "\n",
    "### File 1: testbench_initial.v\n",
    "\n",
    "This is the first-stage testbench that:\n",
    "- Declares all necessary signals\n",
    "- Instantiates the module under test\n",
    "- Applies test patterns systematically\n",
    "- Displays outputs (but doesn't verify them yet)\n",
    "\n",
    "It serves as an intermediate artifact that shows the test structure before verification logic is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the initial testbench (first 50 lines)\n",
    "print(\"Initial Testbench (excerpt):\")\n",
    "print(\"=\" * 70)\n",
    "with open(\"notebook_output/mux/testbench_initial.v\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    print(''.join(lines[:50]))\n",
    "    if len(lines) > 50:\n",
    "        print(f\"\\n... ({len(lines) - 50} more lines)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File 2: golden_model.py\n",
    "\n",
    "The Python golden model is a reference implementation that:\n",
    "- Implements the expected behavior based on the natural language description\n",
    "- Takes the same inputs as the Verilog module\n",
    "- Returns the expected outputs\n",
    "- Can be used independently for verification\n",
    "\n",
    "This model is crucial because it provides a language-agnostic specification of the desired behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the golden model directly\n",
    "print(\"Testing Golden Model Directly:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Import and test the MUX golden model\n",
    "import sys\n",
    "sys.path.insert(0, 'notebook_output/mux')\n",
    "try:\n",
    "    from golden_model import mux2to1_golden\n",
    "    \n",
    "    print(\"\\nMUX Golden Model Test Cases:\")\n",
    "    print(f\"mux2to1_golden(a=0, b=0, sel=0) = {mux2to1_golden(0, 0, 0)}\")\n",
    "    print(f\"mux2to1_golden(a=1, b=0, sel=0) = {mux2to1_golden(1, 0, 0)}\")\n",
    "    print(f\"mux2to1_golden(a=0, b=1, sel=1) = {mux2to1_golden(0, 1, 1)}\")\n",
    "    print(f\"mux2to1_golden(a=1, b=1, sel=1) = {mux2to1_golden(1, 1, 1)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Golden model testing skipped in demo mode: {e}\")\n",
    "finally:\n",
    "    sys.path.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File 3: test_patterns_with_golden.json\n",
    "\n",
    "This JSON file contains:\n",
    "- All test patterns (inputs)\n",
    "- Expected outputs for each pattern (computed by the golden model)\n",
    "- Test numbering for easy reference\n",
    "\n",
    "It serves as a complete test specification that bridges the Python and Verilog domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the test patterns\n",
    "print(\"Test Pattern Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "with open(\"notebook_output/mux/test_patterns_with_golden.json\", \"r\") as f:\n",
    "    patterns = json.load(f)\n",
    "\n",
    "print(f\"Total number of test patterns: {len(patterns)}\")\n",
    "print(f\"\\nPattern structure:\")\n",
    "if patterns:\n",
    "    print(f\"  - test_num: Test identifier\")\n",
    "    print(f\"  - inputs: {list(patterns[0]['inputs'].keys())}\")\n",
    "    print(f\"  - expected_outputs: {list(patterns[0].get('expected_outputs', {}).keys())}\")\n",
    "    \n",
    "    print(f\"\\nExample pattern:\")\n",
    "    print(json.dumps(patterns[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File 4: testbench_final.v\n",
    "\n",
    "The final testbench is the complete, ready-to-simulate file that includes:\n",
    "- Everything from testbench_initial.v\n",
    "- Expected output values for each test\n",
    "- Comparison logic (actual vs. expected)\n",
    "- Pass/fail tracking for each test\n",
    "- Summary report showing total tests passed/failed\n",
    "\n",
    "This is the file you would compile and simulate with a Verilog simulator (Icarus Verilog, ModelSim, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the final testbench (excerpt)\n",
    "print(\"Final Testbench with Verification (excerpt):\")\n",
    "print(\"=\" * 70)\n",
    "with open(\"notebook_output/mux/testbench_final.v\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    # Show first 60 lines to include some verification logic\n",
    "    print(''.join(lines[:60]))\n",
    "    if len(lines) > 60:\n",
    "        print(f\"\\n... ({len(lines) - 60} more lines)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Comparing Testbench Stages\n",
    "\n",
    "### Introduction to Testbench Evolution\n",
    "\n",
    "Let's compare the initial and final testbenches to see what was added in Step 5. This helps understand how the verification logic is integrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare file sizes\n",
    "import os\n",
    "\n",
    "print(\"File Size Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "initial_size = os.path.getsize(\"notebook_output/mux/testbench_initial.v\")\n",
    "final_size = os.path.getsize(\"notebook_output/mux/testbench_final.v\")\n",
    "\n",
    "print(f\"testbench_initial.v: {initial_size} bytes\")\n",
    "print(f\"testbench_final.v:   {final_size} bytes\")\n",
    "print(f\"Additional content:  {final_size - initial_size} bytes\")\n",
    "print(f\"Size increase:       {((final_size / initial_size - 1) * 100):.1f}%\")\n",
    "\n",
    "print(\"\\nThe additional content includes:\")\n",
    "print(\"  - Expected output declarations\")\n",
    "print(\"  - Comparison statements\")\n",
    "print(\"  - Pass/fail counters\")\n",
    "print(\"  - Test summary report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Simulating the Testbench (Optional)\n",
    "\n",
    "### Introduction to Simulation\n",
    "\n",
    "If you have a Verilog simulator installed (like Icarus Verilog), you can actually run the generated testbench to verify the module. This section shows how to do that.\n",
    "\n",
    "**Note**: This requires having `iverilog` and `vvp` installed on your system. If you don't have them, you can skip this section - the testbench generation is complete regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Icarus Verilog is available\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "iverilog_available = shutil.which('iverilog') is not None\n",
    "vvp_available = shutil.which('vvp') is not None\n",
    "\n",
    "if iverilog_available and vvp_available:\n",
    "    print(\"✓ Icarus Verilog is available\")\n",
    "    print(\"  You can simulate the generated testbenches\")\n",
    "else:\n",
    "    print(\"ℹ Icarus Verilog is not installed\")\n",
    "    print(\"  To install it:\")\n",
    "    print(\"    - Ubuntu/Debian: sudo apt-get install iverilog\")\n",
    "    print(\"    - macOS: brew install icarus-verilog\")\n",
    "    print(\"  You can still generate testbenches without simulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the MUX testbench (if simulator is available)\n",
    "if iverilog_available and vvp_available:\n",
    "    print(\"Simulating MUX Testbench:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # First, save the Verilog module to a file\n",
    "    with open(\"notebook_output/mux/mux2to1.v\", \"w\") as f:\n",
    "        f.write(mux_verilog)\n",
    "    \n",
    "    try:\n",
    "        # Compile the testbench\n",
    "        compile_cmd = [\n",
    "            'iverilog',\n",
    "            '-o', 'notebook_output/mux/simulation',\n",
    "            'notebook_output/mux/mux2to1.v',\n",
    "            'notebook_output/mux/testbench_final.v'\n",
    "        ]\n",
    "        result = subprocess.run(compile_cmd, capture_output=True, text=True, timeout=30)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"✓ Compilation successful\")\n",
    "            \n",
    "            # Run the simulation\n",
    "            sim_cmd = ['vvp', 'notebook_output/mux/simulation']\n",
    "            result = subprocess.run(sim_cmd, capture_output=True, text=True, timeout=30)\n",
    "            \n",
    "            print(\"\\nSimulation Output:\")\n",
    "            print(\"-\" * 70)\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(f\"Compilation failed: {result.stderr}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Simulation error: {e}\")\n",
    "else:\n",
    "    print(\"Simulation skipped (Icarus Verilog not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Summary and Next Steps\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "In this notebook, you have:\n",
    "\n",
    "1. ✓ Set up the LLM-aided Testbench Generation environment\n",
    "2. ✓ Understood the 5-step automated pipeline\n",
    "3. ✓ Generated a testbench for a 2-to-1 multiplexer\n",
    "4. ✓ Generated a testbench for a 4-bit adder\n",
    "5. ✓ Examined all generated files and understood their purpose\n",
    "6. ✓ Learned about golden models and test patterns\n",
    "7. ✓ (Optionally) Simulated the testbench with a Verilog simulator\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Automation**: LLMs can significantly reduce manual effort in testbench creation\n",
    "- **Natural Language Specs**: Clear descriptions lead to better test coverage\n",
    "- **Golden Models**: Python reference models provide independent verification\n",
    "- **Systematic Testing**: The pipeline ensures comprehensive test pattern generation\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To continue exploring this system:\n",
    "\n",
    "1. **Try Your Own Modules**: Use the pipeline with your own Verilog designs\n",
    "2. **Experiment with Descriptions**: See how different descriptions affect test generation\n",
    "3. **Add More Complex Modules**: Try sequential circuits or state machines\n",
    "4. **Customize the Pipeline**: Modify the generation logic for specific needs\n",
    "\n",
    "### Using the System from Command Line\n",
    "\n",
    "You can also use the system from the command line:\n",
    "\n",
    "```bash\n",
    "# With example\n",
    "python main.py --example\n",
    "\n",
    "# With custom files\n",
    "python main.py --description my_desc.txt --verilog my_module.v\n",
    "\n",
    "# Run the interactive demo\n",
    "python demo.py\n",
    "```\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- `README.md`: Comprehensive project documentation\n",
    "- `USAGE_GUIDE.md`: Detailed usage instructions and best practices\n",
    "- `PROJECT_SUMMARY.md`: Technical implementation details\n",
    "- `examples/`: Sample input and output files\n",
    "\n",
    "Thank you for using LLM-aided Testbench Generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix: File Listings\n",
    "\n",
    "### All Generated Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all generated files with sizes\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Generated Files Summary:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for example in ['mux', 'adder']:\n",
    "    output_dir = f\"notebook_output/{example}\"\n",
    "    if os.path.exists(output_dir):\n",
    "        print(f\"\\n{example.upper()}:\")\n",
    "        for file in sorted(Path(output_dir).glob('*')):\n",
    "            if file.is_file():\n",
    "                size = file.stat().st_size\n",
    "                print(f\"  - {file.name:30s} ({size:6d} bytes)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
